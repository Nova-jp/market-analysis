#!/usr/bin/env python3
"""
ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ—¥ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å®‰å…¨ãƒãƒ¼ã‚¸ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ä¿è­·ã—ã¤ã¤æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’çµ±åˆ
"""

import json
import sys
from datetime import datetime

def safe_merge_target_dates():
    """å®‰å…¨ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ—¥ä»˜ã‚’ãƒãƒ¼ã‚¸"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
    existing_file = "data_files/target_dates_latest.json"
    new_file = "data_files/target_dates_20250913_011222.json"
    backup_file = f"data_files/target_dates_latest_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    
    try:
        print("ğŸ“Š ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ—¥ä»˜ãƒ•ã‚¡ã‚¤ãƒ«å®‰å…¨ãƒãƒ¼ã‚¸")
        print("=" * 50)
        
        # 1. æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        print("ğŸ“‚ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿...")
        with open(existing_file, 'r', encoding='utf-8') as f:
            existing_data = json.load(f)
        existing_dates = set(existing_data.get('target_dates', []))
        print(f"   æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_dates)}æ—¥åˆ†")
        
        # 2. æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
        print("ğŸ“‚ æ–°è¦ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿...")
        with open(new_file, 'r', encoding='utf-8') as f:
            new_data = json.load(f)
        new_dates = set(new_data.get('target_dates', []))
        print(f"   æ–°è¦ãƒ‡ãƒ¼ã‚¿: {len(new_dates)}æ—¥åˆ†")
        
        # 3. ãƒ‡ãƒ¼ã‚¿åˆ†æ
        print("\nğŸ“ˆ ãƒ‡ãƒ¼ã‚¿åˆ†æ:")
        common_dates = existing_dates & new_dates
        only_existing = existing_dates - new_dates
        only_new = new_dates - existing_dates
        
        print(f"   å…±é€šæ—¥ä»˜: {len(common_dates)}æ—¥åˆ†")
        print(f"   æ—¢å­˜ã®ã¿: {len(only_existing)}æ—¥åˆ†") 
        print(f"   æ–°è¦ã®ã¿: {len(only_new)}æ—¥åˆ†")
        
        if only_existing:
            print(f"   âš ï¸  æ—¢å­˜å°‚ç”¨æ—¥ä»˜ã‚µãƒ³ãƒ—ãƒ«: {sorted(list(only_existing))[:5]}")
        if only_new:
            print(f"   âœ… æ–°è¦è¿½åŠ æ—¥ä»˜ã‚µãƒ³ãƒ—ãƒ«: {sorted(list(only_new), reverse=True)[:5]}")
        
        # 4. ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
        print(f"\nğŸ’¾ æ—¢å­˜ãƒ•ã‚¡ã‚¤ãƒ«ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_file}")
        with open(backup_file, 'w', encoding='utf-8') as f:
            json.dump(existing_data, f, ensure_ascii=False, indent=2)
        
        # 5. ãƒãƒ¼ã‚¸å®Ÿè¡Œ
        merged_dates = sorted(list(existing_dates | new_dates), reverse=True)
        
        # æ–°è¦ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ãƒ™ãƒ¼ã‚¹ã«ä½œæˆï¼ˆã‚ˆã‚Šæ–°ã—ã„timestampç­‰ï¼‰
        merged_data = {
            "collection_timestamp": new_data.get('collection_timestamp'),
            "total_target_dates": len(merged_dates),
            "estimated_time_hours": round(len(merged_dates) * 20 / 3600, 1),
            "merge_info": {
                "existing_dates_count": len(existing_dates),
                "new_dates_count": len(new_dates),
                "merged_dates_count": len(merged_dates),
                "dates_preserved": len(only_existing),
                "dates_added": len(only_new)
            },
            "target_dates": merged_dates
        }
        
        # 6. çµæœä¿å­˜
        print(f"\nğŸ’¾ ãƒãƒ¼ã‚¸çµæœä¿å­˜: {existing_file}")
        with open(existing_file, 'w', encoding='utf-8') as f:
            json.dump(merged_data, f, ensure_ascii=False, indent=2)
        
        print("\nğŸ‰ ãƒãƒ¼ã‚¸å®Œäº†:")
        print(f"   æœ€çµ‚ãƒ‡ãƒ¼ã‚¿: {len(merged_dates)}æ—¥åˆ†")
        print(f"   ãƒ‡ãƒ¼ã‚¿ä¿è­·: {len(only_existing)}æ—¥åˆ†ä¿æŒ")
        print(f"   æ–°è¦è¿½åŠ : {len(only_new)}æ—¥åˆ†")
        print(f"   æ¨å®šæ™‚é–“: {merged_data['estimated_time_hours']}æ™‚é–“")
        
        return True
        
    except Exception as e:
        print(f"âŒ ãƒãƒ¼ã‚¸ã‚¨ãƒ©ãƒ¼: {e}")
        return False

if __name__ == "__main__":
    success = safe_merge_target_dates()
    sys.exit(0 if success else 1)