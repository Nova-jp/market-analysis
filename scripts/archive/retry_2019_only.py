#!/usr/bin/env python3
"""
2019å¹´ã®ã¿ã®å€‹åˆ¥ãƒªãƒˆãƒ©ã‚¤ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ã‚ˆã‚Šé•·ã„ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ60ç§’ï¼‰ã¨è¤‡æ•°å›ãƒªãƒˆãƒ©ã‚¤ã§å¯¾å¿œ
"""

import sys
import os
import json
import time
import requests
from datetime import datetime
from bs4 import BeautifulSoup

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def retry_2019_only():
    """2019å¹´ãƒ‡ãƒ¼ã‚¿ã®å°‚ç”¨å–å¾—"""
    base_url = "https://market.jsda.or.jp"
    year = 2019
    
    print("ğŸ”„ 2019å¹´å°‚ç”¨ãƒ‡ãƒ¼ã‚¿å–å¾—ï¼ˆ60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã€5å›ãƒªãƒˆãƒ©ã‚¤ï¼‰")
    print("=" * 60)
    
    archive_url = f"{base_url}/shijyo/saiken/baibai/baisanchi/archive{year}.html"
    max_retries = 5
    timeout_seconds = 60  # 60ç§’ã«å»¶é•·
    
    for attempt in range(max_retries):
        try:
            print(f"\nğŸ“… 2019å¹´ãƒ‡ãƒ¼ã‚¿å–å¾— - è©¦è¡Œ {attempt + 1}/{max_retries}")
            print(f"  ğŸŒ URL: {archive_url}")
            print(f"  â±ï¸  {timeout_seconds}ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š...")
            
            # 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§å–å¾—
            response = requests.get(archive_url, timeout=timeout_seconds)
            response.raise_for_status()
            
            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            # HTMLãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.text, 'html.parser')
            year_dates = []
            
            # CSVãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and ('S' in href and '.csv' in href):
                    import re
                    match = re.search(r'S(\d{6})\.csv', href)
                    if match:
                        date_str = match.group(1)
                        try:
                            if len(date_str) == 6:
                                parsed_year = int('20' + date_str[:2])
                                month = int(date_str[2:4])
                                day = int(date_str[4:6])
                                
                                # 2019å¹´ã¨ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
                                if parsed_year == year:
                                    date_obj = f"{parsed_year}-{month:02d}-{day:02d}"
                                    year_dates.append(date_obj)
                        except ValueError:
                            continue
            
            if year_dates:
                sorted_dates = sorted(year_dates)
                print(f"  âœ… 2019å¹´: {len(year_dates)}æ—¥åˆ†å–å¾—æˆåŠŸ")
                print(f"     ã‚µãƒ³ãƒ—ãƒ«: {sorted_dates[:3]}...{sorted_dates[-3:]}")
                
                # çµæœä¿å­˜
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                result_file = f"data_files/recovered_2019_{timestamp}.json"
                
                result_data = {
                    "recovery_timestamp": timestamp,
                    "year": 2019,
                    "attempt_number": attempt + 1,
                    "timeout_used": timeout_seconds,
                    "dates_count": len(year_dates),
                    "dates": sorted_dates
                }
                
                with open(result_file, 'w', encoding='utf-8') as f:
                    json.dump(result_data, f, ensure_ascii=False, indent=2)
                
                print(f"\nğŸ’¾ 2019å¹´ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {result_file}")
                print(f"ğŸ‰ å–å¾—æˆåŠŸ: {len(year_dates)}æ—¥åˆ†")
                print(f"ğŸ“… æœŸé–“: {sorted_dates[0]} ï½ {sorted_dates[-1]}")
                
                return result_file, result_data
            else:
                print(f"  âŒ 2019å¹´: ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆè©¦è¡Œ {attempt + 1}ï¼‰")
                
        except requests.exceptions.Timeout:
            print(f"  âš ï¸  2019å¹´: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ{timeout_seconds}ç§’çµŒéã€è©¦è¡Œ {attempt + 1}ï¼‰")
        except requests.exceptions.ConnectionError:
            print(f"  âš ï¸  2019å¹´: æ¥ç¶šã‚¨ãƒ©ãƒ¼ï¼ˆè©¦è¡Œ {attempt + 1}ï¼‰")
        except Exception as e:
            print(f"  âŒ 2019å¹´: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ï¼ˆè©¦è¡Œ {attempt + 1}ï¼‰ - {e}")
        
        # æ¬¡ã®è©¦è¡Œå‰ã«å¾…æ©Ÿï¼ˆæœ€å¾Œã®è©¦è¡Œä»¥å¤–ï¼‰
        if attempt < max_retries - 1:
            wait_time = 45  # 45ç§’å¾…æ©Ÿ
            print(f"  â³ {wait_time}ç§’å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
            time.sleep(wait_time)
    
    print(f"\nâŒ 2019å¹´: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤æ•°ï¼ˆ{max_retries}å›ï¼‰åˆ°é”")
    return None, {}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš¨ JSDAä¿è­·ãƒ«ãƒ¼ãƒ«:")
    print("  - 45ç§’é–“éš”ã§ãƒªãƒˆãƒ©ã‚¤")
    print("  - 60ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
    print("  - æœ€å¤§5å›ãƒªãƒˆãƒ©ã‚¤")
    print()
    
    try:
        result_file, result_data = retry_2019_only()
        
        if result_data:
            print(f"\nâœ… 2019å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†:")
            print(f"  - ãƒ•ã‚¡ã‚¤ãƒ«: {result_file}")
            print(f"  - æ—¥æ•°: {result_data['dates_count']}æ—¥")
            print(f"  - æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—: target_dates_latest.jsonã«çµ±åˆ")
            return 0
        else:
            print(f"\nâŒ 2019å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—ã«å¤±æ•—")
            print("  - JSDAã‚µãƒ¼ãƒãƒ¼ã®è² è·ãŒé«˜ã„å¯èƒ½æ€§")
            print("  - å¾Œæ—¥å†è©¦è¡Œã‚’æ¨å¥¨")
            return 1
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        print(f"\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())