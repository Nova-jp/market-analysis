#!/usr/bin/env python3
"""
2019å¹´ãƒ‡ãƒ¼ã‚¿ã‚’target_dates_latest.jsonã«çµ±åˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from datetime import datetime
from typing import Set, List

def merge_2019_data():
    """2019å¹´ãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ã®target_datesã«å®‰å…¨ã«çµ±åˆ"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ“– ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    
    # æ—¢å­˜ã®target_dates_latest.json
    with open("data_files/target_dates_latest.json", 'r', encoding='utf-8') as f:
        existing_data = json.load(f)
    
    # 2019å¹´ãƒ‡ãƒ¼ã‚¿
    with open("data_files/recovered_2019_20250913_040248.json", 'r', encoding='utf-8') as f:
        data_2019 = json.load(f)
    
    print(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {existing_data['total_target_dates']}æ—¥åˆ†")
    print(f"âœ… 2019å¹´ãƒ‡ãƒ¼ã‚¿: {data_2019['dates_count']}æ—¥åˆ†")
    
    # æ—¢å­˜æ—¥ä»˜ã‚’ã‚»ãƒƒãƒˆã«å¤‰æ›
    existing_dates: Set[str] = set(existing_data['target_dates'])
    print(f"ğŸ“Š æ—¢å­˜æ—¥ä»˜æ•°: {len(existing_dates)}")
    
    # 2019å¹´ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
    dates_2019: Set[str] = set(data_2019['dates'])
    print(f"ğŸ“Š 2019å¹´æ—¥ä»˜æ•°: {len(dates_2019)}")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates = existing_dates & dates_2019
    print(f"ğŸ”„ é‡è¤‡æ—¥ä»˜æ•°: {len(duplicates)}")
    
    # æ–°è¦è¿½åŠ åˆ†
    new_dates = dates_2019 - existing_dates
    print(f"ğŸ†• æ–°è¦è¿½åŠ åˆ†: {len(new_dates)}")
    
    if len(new_dates) == 0:
        print("â„¹ï¸  è¿½åŠ ã™ã‚‹æ–°ã—ã„æ—¥ä»˜ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    if len(new_dates) > 0:
        sorted_new = sorted(list(new_dates))
        print(f"ğŸ“… æ–°è¦æ—¥ä»˜ã‚µãƒ³ãƒ—ãƒ«: {sorted_new[:5]}...{sorted_new[-5:]}")
    
    # çµ±åˆ
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
    all_dates = existing_dates | dates_2019
    final_dates = sorted(list(all_dates), reverse=True)  # é™é †ã‚½ãƒ¼ãƒˆ
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    merged_data = {
        "collection_timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "total_target_dates": len(final_dates),
        "estimated_time_hours": round(len(final_dates) * 5.5 / 3600, 1),
        "merge_info": {
            "existing_dates_count": len(existing_dates),
            "dates_2019_count": len(dates_2019),
            "new_dates_count": len(new_dates),
            "duplicates_count": len(duplicates),
            "merged_dates_count": len(final_dates),
            "dates_preserved": len(existing_dates),
            "dates_added": len(new_dates)
        },
        "recovery_info": {
            "previous_recoveries": existing_data.get("recovery_info", {}),
            "latest_recovery": {
                "source_file": "recovered_2019_20250913_040248.json",
                "year": 2019,
                "dates_added": len(new_dates),
                "merge_timestamp": datetime.now().strftime("%Y%m%d_%H%M%S")
            }
        },
        "target_dates": final_dates
    }
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_filename = f"data_files/target_dates_backup_before_2019_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(backup_filename, 'w', encoding='utf-8') as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_filename}")
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
    with open("data_files/target_dates_latest.json", 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… 2019å¹´ãƒ‡ãƒ¼ã‚¿çµ±åˆå®Œäº†!")
    print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"   - å…ƒãƒ‡ãƒ¼ã‚¿: {len(existing_dates)}æ—¥")
    print(f"   - 2019å¹´ãƒ‡ãƒ¼ã‚¿: {len(dates_2019)}æ—¥")
    print(f"   - é‡è¤‡é™¤å¤–å¾Œ: {len(final_dates)}æ—¥")
    print(f"   - æ–°è¦è¿½åŠ : {len(new_dates)}æ—¥")
    print(f"   - æœ€å¤æ—¥ä»˜: {final_dates[-1]}")
    print(f"   - æœ€æ–°æ—¥ä»˜: {final_dates[0]}")
    
    return merged_data

if __name__ == "__main__":
    merge_2019_data()