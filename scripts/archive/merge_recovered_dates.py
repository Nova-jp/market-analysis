#!/usr/bin/env python3
"""
å›å¾©ãƒ‡ãƒ¼ã‚¿ã‚’target_dates_latest.jsonã«çµ±åˆã™ã‚‹ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
"""

import json
from datetime import datetime
from typing import Set, List

def merge_recovered_dates():
    """å›å¾©ã—ãŸãƒ‡ãƒ¼ã‚¿ã‚’æ—¢å­˜ã®target_datesã«å®‰å…¨ã«çµ±åˆ"""
    
    # ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿
    print("ğŸ“– ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ä¸­...")
    
    # æ—¢å­˜ã®target_dates_latest.json
    with open("data_files/target_dates_latest.json", 'r', encoding='utf-8') as f:
        existing_data = json.load(f)
    
    # å›å¾©ãƒ‡ãƒ¼ã‚¿
    with open("data_files/recovered_dates_20250913_035123.json", 'r', encoding='utf-8') as f:
        recovered_data = json.load(f)
    
    print(f"âœ… æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {existing_data['total_target_dates']}æ—¥åˆ†")
    print(f"âœ… å›å¾©ãƒ‡ãƒ¼ã‚¿: {recovered_data['summary']} = {sum(recovered_data['summary'].values())}æ—¥åˆ†")
    
    # æ—¢å­˜æ—¥ä»˜ã‚’ã‚»ãƒƒãƒˆã«å¤‰æ›
    existing_dates: Set[str] = set(existing_data['target_dates'])
    print(f"ğŸ“Š æ—¢å­˜æ—¥ä»˜æ•°: {len(existing_dates)}")
    
    # å›å¾©ãƒ‡ãƒ¼ã‚¿ã‹ã‚‰æ—¥ä»˜ã‚’æŠ½å‡º
    recovered_dates: Set[str] = set()
    for year_str, dates_list in recovered_data['recovered_data'].items():
        recovered_dates.update(dates_list)
    
    print(f"ğŸ“Š å›å¾©æ—¥ä»˜æ•°: {len(recovered_dates)}")
    
    # é‡è¤‡ãƒã‚§ãƒƒã‚¯
    duplicates = existing_dates & recovered_dates
    print(f"ğŸ”„ é‡è¤‡æ—¥ä»˜æ•°: {len(duplicates)}")
    
    # æ–°è¦è¿½åŠ åˆ†
    new_dates = recovered_dates - existing_dates
    print(f"ğŸ†• æ–°è¦è¿½åŠ åˆ†: {len(new_dates)}")
    
    if len(new_dates) == 0:
        print("â„¹ï¸  è¿½åŠ ã™ã‚‹æ–°ã—ã„æ—¥ä»˜ãŒã‚ã‚Šã¾ã›ã‚“")
        return
    
    # ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    if len(new_dates) > 0:
        sorted_new = sorted(list(new_dates))
        print(f"ğŸ“… æ–°è¦æ—¥ä»˜ã‚µãƒ³ãƒ—ãƒ«: {sorted_new[:5]}...{sorted_new[-5:]}")
    
    # çµ±åˆ
    print("ğŸ”„ ãƒ‡ãƒ¼ã‚¿çµ±åˆä¸­...")
    all_dates = existing_dates | recovered_dates
    final_dates = sorted(list(all_dates), reverse=True)  # é™é †ã‚½ãƒ¼ãƒˆ
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ä½œæˆ
    merged_data = {
        "collection_timestamp": datetime.now().strftime("%Y%m%d_%H%M%S"),
        "total_target_dates": len(final_dates),
        "estimated_time_hours": round(len(final_dates) * 5.5 / 3600, 1),
        "merge_info": {
            "existing_dates_count": len(existing_dates),
            "recovered_dates_count": len(recovered_dates),
            "new_dates_count": len(new_dates),
            "duplicates_count": len(duplicates),
            "merged_dates_count": len(final_dates),
            "dates_preserved": len(existing_dates),
            "dates_added": len(new_dates)
        },
        "recovery_info": {
            "source_file": "recovered_dates_20250913_035123.json",
            "recovered_years": list(recovered_data['recovered_data'].keys()),
            "years_recovered": recovered_data['total_recovered_years']
        },
        "target_dates": final_dates
    }
    
    # ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—ä½œæˆ
    backup_filename = f"data_files/target_dates_backup_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    with open(backup_filename, 'w', encoding='utf-8') as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)
    print(f"ğŸ’¾ æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ãƒãƒƒã‚¯ã‚¢ãƒƒãƒ—: {backup_filename}")
    
    # çµ±åˆãƒ‡ãƒ¼ã‚¿ä¿å­˜
    with open("data_files/target_dates_latest.json", 'w', encoding='utf-8') as f:
        json.dump(merged_data, f, ensure_ascii=False, indent=2)
    
    print(f"âœ… çµ±åˆå®Œäº†!")
    print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
    print(f"   - å…ƒãƒ‡ãƒ¼ã‚¿: {len(existing_dates)}æ—¥")
    print(f"   - å›å¾©ãƒ‡ãƒ¼ã‚¿: {len(recovered_dates)}æ—¥")
    print(f"   - é‡è¤‡é™¤å¤–å¾Œ: {len(final_dates)}æ—¥")
    print(f"   - æ–°è¦è¿½åŠ : {len(new_dates)}æ—¥")
    print(f"   - æœ€å¤æ—¥ä»˜: {final_dates[-1]}")
    print(f"   - æœ€æ–°æ—¥ä»˜: {final_dates[0]}")
    
    return merged_data

if __name__ == "__main__":
    merge_recovered_dates()