#!/usr/bin/env python3
"""
JSDAã®2008-2019å¹´ãƒ‡ãƒ¼ã‚¿å†å–å¾—ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
30ç§’é–“éš”ã§ãƒªãƒˆãƒ©ã‚¤å®Ÿè¡Œ
"""

import sys
import os
import json
import time
import requests
from datetime import datetime
from bs4 import BeautifulSoup

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

def retry_missing_years():
    """æ¬ æå¹´ãƒ‡ãƒ¼ã‚¿ã®å†å–å¾—"""
    base_url = "https://market.jsda.or.jp"
    missing_years = [2008, 2009, 2010, 2011, 2012, 2013, 2014, 2015, 2016, 2017, 2018, 2019]
    
    print("ğŸ”„ JSDAæ¬ æå¹´ãƒ‡ãƒ¼ã‚¿å†å–å¾—ï¼ˆ30ç§’é–“éš”ï¼‰")
    print("=" * 50)
    
    recovered_dates = {}
    
    for year in missing_years:
        print(f"\nğŸ“… {year}å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹...")
        archive_url = f"{base_url}/shijyo/saiken/baibai/baisanchi/archive{year}.html"
        
        try:
            print(f"  ğŸŒ URL: {archive_url}")
            print("  â±ï¸  45ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆè¨­å®š...")
            
            # 45ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã§å–å¾—
            response = requests.get(archive_url, timeout=45)
            response.raise_for_status()
            
            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            # HTMLãƒ‘ãƒ¼ã‚¹
            soup = BeautifulSoup(response.text, 'html.parser')
            year_dates = []
            
            # CSVãƒªãƒ³ã‚¯ã‚’æ¤œç´¢
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and ('S' in href and '.csv' in href):
                    import re
                    match = re.search(r'S(\d{6})\.csv', href)
                    if match:
                        date_str = match.group(1)
                        try:
                            if len(date_str) == 6:
                                parsed_year = int('20' + date_str[:2])
                                month = int(date_str[2:4])
                                day = int(date_str[4:6])
                                
                                # æŒ‡å®šå¹´ã¨ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
                                if parsed_year == year:
                                    date_obj = f"{parsed_year}-{month:02d}-{day:02d}"
                                    year_dates.append(date_obj)
                        except ValueError:
                            continue
            
            if year_dates:
                recovered_dates[year] = sorted(year_dates)
                print(f"  âœ… {year}å¹´: {len(year_dates)}æ—¥åˆ†å–å¾—æˆåŠŸ")
                print(f"     ã‚µãƒ³ãƒ—ãƒ«: {year_dates[:3]}...")
            else:
                print(f"  âŒ {year}å¹´: ãƒ‡ãƒ¼ã‚¿ãªã—")
                
        except requests.exceptions.Timeout:
            print(f"  âš ï¸  {year}å¹´: ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆï¼ˆ45ç§’çµŒéï¼‰")
        except requests.exceptions.ConnectionError:
            print(f"  âš ï¸  {year}å¹´: æ¥ç¶šã‚¨ãƒ©ãƒ¼")
        except Exception as e:
            print(f"  âŒ {year}å¹´: äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼ - {e}")
        
        # 30ç§’å¾…æ©Ÿï¼ˆæœ€å¾Œã®å¹´ä»¥å¤–ï¼‰
        if year < missing_years[-1]:
            print("  â³ 30ç§’å¾…æ©Ÿä¸­...")
            time.sleep(30)
    
    # çµæœä¿å­˜
    if recovered_dates:
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        result_file = f"data_files/recovered_dates_{timestamp}.json"
        
        with open(result_file, 'w', encoding='utf-8') as f:
            json.dump({
                "recovery_timestamp": timestamp,
                "total_recovered_years": len(recovered_dates),
                "recovered_data": recovered_dates,
                "summary": {
                    year: len(dates) for year, dates in recovered_dates.items()
                }
            }, f, ensure_ascii=False, indent=2)
        
        print(f"\nğŸ’¾ å›å¾©ãƒ‡ãƒ¼ã‚¿ä¿å­˜: {result_file}")
        print(f"ğŸ‰ å›å¾©å®Œäº†: {len(recovered_dates)}å¹´åˆ†")
        
        # çµ±è¨ˆè¡¨ç¤º
        total_dates = sum(len(dates) for dates in recovered_dates.values())
        print(f"ğŸ“Š ç·ãƒ‡ãƒ¼ã‚¿æ•°: {total_dates}æ—¥åˆ†")
        
        return result_file, recovered_dates
    else:
        print("\nâŒ å›å¾©ãƒ‡ãƒ¼ã‚¿ãªã—")
        return None, {}

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    print("ğŸš¨ JSDAä¿è­·ãƒ«ãƒ¼ãƒ«:")
    print("  - 30ç§’é–“éš”ã§ã‚¢ã‚¯ã‚»ã‚¹")
    print("  - 45ç§’ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
    print("  - ãƒ•ã‚©ã‚¢ã‚°ãƒ©ã‚¦ãƒ³ãƒ‰å®Ÿè¡Œ")
    print()
    
    try:
        result_file, recovered_data = retry_missing_years()
        
        if recovered_data:
            print(f"\nâœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—:")
            print(f"  1. {result_file} ã‚’ç¢ºèª")
            print(f"  2. target_dates_latest.json ã«çµ±åˆ")
            return 0
        else:
            print(f"\nâŒ ãƒ‡ãƒ¼ã‚¿å›å¾©ã«å¤±æ•—")
            return 1
            
    except KeyboardInterrupt:
        print("\n\nâš ï¸ ãƒ¦ãƒ¼ã‚¶ãƒ¼ã«ã‚ˆã£ã¦ä¸­æ–­ã•ã‚Œã¾ã—ãŸ")
        return 1
    except Exception as e:
        print(f"\nğŸ’¥ äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {e}")
        return 1

if __name__ == "__main__":
    sys.exit(main())