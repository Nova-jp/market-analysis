#!/usr/bin/env python3
"""
ã‚·ãƒ³ãƒ—ãƒ«è¤‡æ•°æ—¥ãƒ‡ãƒ¼ã‚¿åé›†ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
collect_single_day.pyã‚’ãƒ™ãƒ¼ã‚¹ã«ä½œæˆ
"""

import sys
import os
import json
import time
from datetime import datetime
import logging

# ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‚’ãƒ‘ã‚¹ã«è¿½åŠ 
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from data.processors.bond_data_processor import BondDataProcessor
from data.utils.database_manager import DatabaseManager

def setup_logging():
    """ãƒ­ã‚°è¨­å®š"""
    logging.basicConfig(
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        handlers=[
            logging.StreamHandler(),
            logging.FileHandler(f'data_collection_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log')
        ]
    )
    return logging.getLogger(__name__)

def load_target_dates(json_file_path):
    """JSONãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰å¯¾è±¡æ—¥ä»˜ã‚’èª­ã¿è¾¼ã¿"""
    try:
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        target_dates = data.get('target_dates', [])
        logger.info(f"ğŸ“‚ ã‚¿ãƒ¼ã‚²ãƒƒãƒˆæ—¥ä»˜èª­ã¿è¾¼ã¿: {len(target_dates)}æ—¥åˆ†")
        return target_dates
    
    except Exception as e:
        logger.error(f"âŒ JSONãƒ•ã‚¡ã‚¤ãƒ«èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
        return []

def collect_single_day_data(processor, db_manager, target_date_str, retry_count=1):
    """
    1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆcollect_single_day.pyã¨åŒã˜ãƒ­ã‚¸ãƒƒã‚¯ï¼‰
    """
    from datetime import date
    
    max_retries = retry_count
    
    for attempt in range(max_retries + 1):
        try:
            logger.info(f"ğŸ“… åé›†ä¸­: {target_date_str} (è©¦è¡Œ {attempt + 1}/{max_retries + 1})")
            
            # æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
            target_date = date.fromisoformat(target_date_str)
            
            # 1. CSVãƒ‡ãƒ¼ã‚¿å–å¾—
            url, filename = processor.build_csv_url(target_date)
            raw_df = processor.download_csv_data(url)
            
            if raw_df is None:
                logger.warning(f"  âŒ CSVãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {target_date_str}")
                continue
            
            if raw_df.empty:
                logger.info(f"  ğŸ“­ ãƒ‡ãƒ¼ã‚¿ãŒç©ºï¼ˆä¼‘æ—¥ã®å¯èƒ½æ€§ï¼‰: {target_date_str}")
                return 0
            
            # 2. ãƒ‡ãƒ¼ã‚¿å‡¦ç†
            processed_df = processor.process_raw_data(raw_df)
            
            if processed_df.empty:
                logger.info(f"  ğŸ“­ å‡¦ç†å¾Œãƒ‡ãƒ¼ã‚¿ãŒç©º: {target_date_str}")
                return 0
            
            # 3. trade_dateè¿½åŠ 
            processed_df['trade_date'] = target_date_str
            
            # 4. ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
            batch_data = processed_df.to_dict('records')
            saved_count = db_manager.batch_insert_data(batch_data)
            
            if saved_count > 0:
                logger.info(f"  âœ… æˆåŠŸ: {saved_count}ä»¶ä¿å­˜")
                return saved_count
            else:
                logger.warning(f"  âš ï¸  ä¿å­˜å¤±æ•—: {target_date_str}")
                return 0
                
        except Exception as e:
            error_msg = str(e)
            is_timeout = "timeout" in error_msg.lower() or "connection" in error_msg.lower()
            
            if attempt < max_retries:
                if is_timeout:
                    logger.warning(f"  â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {error_msg}")
                    logger.info(f"  â±ï¸  5åˆ†å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(300)  # 5åˆ†å¾…æ©Ÿï¼ˆã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚ï¼‰
                else:
                    logger.warning(f"  âŒ ã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}): {error_msg}")
                    logger.info(f"  â±ï¸  3åˆ†å¾…æ©Ÿã—ã¦ãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(180)  # 3åˆ†å¾…æ©Ÿï¼ˆé€šå¸¸ã‚¨ãƒ©ãƒ¼ï¼‰
            else:
                logger.error(f"  âŒ æœ€çµ‚çš„ã«å¤±æ•—: {target_date_str} - {error_msg}")
                if is_timeout:
                    logger.info(f"  â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã®ãŸã‚5åˆ†å¾…æ©Ÿå¾Œã€æ¬¡ã®æ—¥ä»˜ã¸")
                    time.sleep(300)  # 5åˆ†å¾…æ©Ÿã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                else:
                    logger.info(f"  â­ï¸  30ç§’å¾…æ©Ÿå¾Œã€æ¬¡ã®æ—¥ä»˜ã¸ã‚¹ã‚­ãƒƒãƒ—")
                    time.sleep(30)  # 30ç§’å¾…æ©Ÿã—ã¦ã‚¹ã‚­ãƒƒãƒ—
                return None
    
    return None

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    if len(sys.argv) != 2:
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python simple_multi_day_collector.py <target_dates_json_file>")
        print("\nä¾‹:")
        print("  python simple_multi_day_collector.py data_files/target_dates_latest.json")
        return 1
    
    json_file_path = sys.argv[1]
    global logger
    logger = setup_logging()
    
    logger.info("ğŸ¯ ã‚·ãƒ³ãƒ—ãƒ«è¤‡æ•°æ—¥ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆJSDAã‚µãƒ¼ãƒãƒ¼ä¿è­·ç‰ˆï¼‰")
    logger.info("=" * 60)
    logger.info("ğŸ“Š Jåˆ—: interest_payment_date (MM/DDæ–‡å­—åˆ—å½¢å¼)")
    logger.info("ğŸ”„ å‡¦ç†: 1æ—¥ã”ã¨ã«ä¿å­˜")
    logger.info("â±ï¸  æ—¥ä»˜é–“éš”: 30ç§’å¾…æ©Ÿ")
    logger.info("â±ï¸  ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆæ™‚: 5åˆ†å¾…æ©Ÿâ†’ãƒªãƒˆãƒ©ã‚¤â†’5åˆ†å¾…æ©Ÿâ†’ã‚¹ã‚­ãƒƒãƒ—")
    logger.info("â±ï¸  é€šå¸¸ã‚¨ãƒ©ãƒ¼æ™‚: 3åˆ†å¾…æ©Ÿâ†’ãƒªãƒˆãƒ©ã‚¤â†’30ç§’å¾…æ©Ÿâ†’ã‚¹ã‚­ãƒƒãƒ—")
    logger.info("-" * 60)
    
    # å¯¾è±¡æ—¥ä»˜èª­ã¿è¾¼ã¿
    target_dates = load_target_dates(json_file_path)
    if not target_dates:
        logger.error("âŒ å¯¾è±¡æ—¥ä»˜ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        return 1
    
    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ç®¡ç†ã¨ãƒ—ãƒ­ã‚»ãƒƒã‚µåˆæœŸåŒ–
    db_manager = DatabaseManager()
    processor = BondDataProcessor()
    
    # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    logger.info("ğŸ“Š æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèªä¸­...")
    existing_dates = db_manager.get_all_existing_dates()
    logger.info(f"ğŸ“Š DBæ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(existing_dates)}æ—¥åˆ†")
    
    # æœªåé›†æ—¥ä»˜ã®ç‰¹å®š
    new_dates = [date for date in target_dates if date not in existing_dates]
    logger.info(f"ğŸ¯ æ–°è¦åé›†å¯¾è±¡: {len(new_dates)}æ—¥åˆ†")
    
    if not new_dates:
        logger.info("âœ… å…¨ã¦ã®å¯¾è±¡æ—¥ä»˜ã¯æ—¢ã«åé›†æ¸ˆã¿ã§ã™")
        return 0
    
    # ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹
    logger.info("ğŸš€ ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹...")
    logger.info("-" * 60)
    
    successful_count = 0
    failed_count = 0
    skipped_count = 0
    
    for i, target_date in enumerate(new_dates, 1):
        logger.info(f"[{i:3d}/{len(new_dates)}] å‡¦ç†ä¸­: {target_date}")
        
        # 1æ—¥åˆ†ã®ãƒ‡ãƒ¼ã‚¿åé›†
        result = collect_single_day_data(processor, db_manager, target_date)
        
        if result is not None:
            if result > 0:
                successful_count += 1
                logger.info(f"  ğŸ“ˆ ç´¯è¨ˆæˆåŠŸ: {successful_count}æ—¥")
            else:
                skipped_count += 1
                logger.info(f"  ğŸ“Š ãƒ‡ãƒ¼ã‚¿ãªã— (ç´¯è¨ˆ: {skipped_count}æ—¥)")
        else:
            failed_count += 1
            logger.info(f"  ğŸ“‰ ç´¯è¨ˆå¤±æ•—: {failed_count}æ—¥")
        
        # æ¬¡ã®æ—¥ä»˜ã¸ã®é–“éš”ï¼ˆJSDAã‚µãƒ¼ãƒãƒ¼ä¿è­·ã®ãŸã‚30ç§’ï¼‰
        if i < len(new_dates):
            logger.info(f"  â±ï¸  30ç§’å¾…æ©Ÿã—ã¦ã‹ã‚‰æ¬¡ã®æ—¥ä»˜ã¸...")
            time.sleep(30)  # 30ç§’é–“éš”ã§æ¬¡ã¸
    
    # çµæœã‚µãƒãƒªãƒ¼
    logger.info("=" * 60)
    logger.info("ğŸ‰ ãƒ‡ãƒ¼ã‚¿åé›†å®Œäº†")
    logger.info(f"âœ… æˆåŠŸ: {successful_count}æ—¥")
    logger.info(f"âš ï¸  ãƒ‡ãƒ¼ã‚¿ãªã—: {skipped_count}æ—¥")
    logger.info(f"âŒ å¤±æ•—: {failed_count}æ—¥")
    logger.info(f"ğŸ“Š å‡¦ç†å¯¾è±¡: {len(new_dates)}æ—¥")
    
    return 0

if __name__ == "__main__":
    sys.exit(main())