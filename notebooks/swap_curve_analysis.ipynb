{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Swap Curve Historical Analysis & Cheap/Rich Valuation (Interactive)\n",
    "\n",
    "## 概要\n",
    "過去100日間のOISデータを用いて、以下の分析をインタラクティブに行います。\n",
    "\n",
    "1. **データ取得 & グリッド生成**: 0.5年刻みの高解像度Forward Gridを作成\n",
    "2. **PCAモデル構築**: 全期間データでPCAモデルを学習し、カーブの「構造的特徴」を抽出\n",
    "3. **インタラクティブ分析**: スライダーで過去の日付を選択し、その時点での「Z-Score（統計的割高・割安）」と「PCA残差（構造的割高・割安）」をヒートマップで確認"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 基本ライブラリ\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import QuantLib as ql\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact\n",
    "\n",
    "project_root = Path(\"../\").resolve()\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from sqlalchemy import create_engine, text\n",
    "try:\n",
    "    from app.core.config import settings\n",
    "    DATABASE_URL = settings.database_url\n",
    "    engine = create_engine(DATABASE_URL)\n",
    "except Exception as e:\n",
    "    print(f\"DB Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. データ取得 & グリッド生成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_historical_irs_data(limit_days=100):\n",
    "    with engine.connect() as conn:\n",
    "        # SQL using single quotes, no escaping needed in JSON\n",
    "        date_query = text(\"SELECT DISTINCT trade_date FROM irs_data WHERE product_type = 'OIS' ORDER BY trade_date DESC LIMIT :limit\")\n",
    "        dates_df = pd.read_sql(date_query, conn, params={'limit': limit_days})\n",
    "        target_dates = tuple(dates_df['trade_date'].tolist())\n",
    "        if not target_dates: return pd.DataFrame()\n",
    "        dates_param = target_dates if len(target_dates) > 1 else (target_dates[0],)\n",
    "        data_query = text(\"SELECT trade_date, tenor, rate FROM irs_data WHERE product_type = 'OIS' AND trade_date IN :dates\")\n",
    "        df = pd.read_sql(data_query, conn, params={'dates': dates_param})\n",
    "    return df\n",
    "\n",
    "def convert_tenor(t): \n",
    "    if 'Y' in t: return ql.Period(int(t.replace('Y', '')), ql.Years)\n",
    "    if 'M' in t: return ql.Period(int(t.split('(')[0].replace('M', '')), ql.Months)\n",
    "    return None\n",
    "\n",
    "calc_starts = np.arange(0.5, 10.5, 0.5).tolist()\n",
    "calc_tenors = np.arange(0.5, 10.5, 0.5).tolist() + [15.0, 20.0, 30.0]\n",
    "\n",
    "df_hist = get_historical_irs_data(100)\n",
    "history_records = []\n",
    "unique_dates = sorted(df_hist['trade_date'].unique())\n",
    "\n",
    "for d in unique_dates:\n",
    "    day_data = df_hist[df_hist['trade_date'] == d]\n",
    "    try:\n",
    "        val_date = ql.Date(d.day, d.month, d.year)\n",
    "        ql.Settings.instance().evaluationDate = val_date\n",
    "        helpers = [ql.OISRateHelper(2, convert_tenor(row['tenor']), ql.QuoteHandle(ql.SimpleQuote(row['rate']/100.0)), ql.OvernightIndex(\"TONA\", 2, ql.JPYCurrency(), ql.Japan(), ql.Actual365Fixed())) for _, row in day_data.iterrows() if convert_tenor(row['tenor'])]\n",
    "        curve = ql.PiecewiseLogCubicDiscount(val_date, helpers, ql.Actual365Fixed())\n",
    "        curve.enableExtrapolation()\n",
    "        \n",
    "        res = {'trade_date': d}\n",
    "        for s in calc_starts:\n",
    "            for t in calc_tenors:\n",
    "                sd = val_date + ql.Period(int(s*12), ql.Months)\n",
    "                res[f\"{s}Y_{t}Y\"] = curve.forwardRate(sd, sd + ql.Period(int(t*12), ql.Months), ql.Actual365Fixed(), ql.Compounded, ql.Annual).rate() * 100\n",
    "        history_records.append(res)\n",
    "    except: pass\n",
    "\n",
    "df_fwd = pd.DataFrame(history_records).set_index('trade_date').sort_index()\n",
    "print(f\"Grid Ready: {df_fwd.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. PCAモデル構築 & 全期間の残差計算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. PCA学習 (全期間)\n",
    "X = df_fwd.dropna()\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "pca = PCA(n_components=3)\n",
    "pca.fit(X_scaled)\n",
    "\n",
    "# 2. 全期間の残差計算\n",
    "X_reconstructed = scaler.inverse_transform(pca.inverse_transform(pca.transform(X_scaled)))\n",
    "df_residuals = pd.DataFrame(X.values - X_reconstructed, index=X.index, columns=X.columns) * 100 # bp単位\n",
    "\n",
    "# 3. 全期間のZ-Score計算\n",
    "df_zscores = (df_fwd - df_fwd.mean()) / df_fwd.std()\n",
    "\n",
    "print(f\"PCA Explained Variance: {pca.explained_variance_ratio_}\")\n",
    "print(f\"Residuals & Z-Scores pre-calculated for all {len(X)} days.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. インタラクティブ・ダッシュボード"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_dashboard(date_idx):\n",
    "    target_date = df_fwd.index[date_idx]\n",
    "    z_data = df_zscores.loc[target_date]\n",
    "    res_data = df_residuals.loc[target_date]\n",
    "    \n",
    "    def to_matrix(series):\n",
    "        mat = pd.DataFrame(index=calc_starts, columns=calc_tenors)\n",
    "        for s in calc_starts:\n",
    "            for t in calc_tenors:\n",
    "                col = f\"{s}Y_{t}Y\"\n",
    "                if col in series: mat.loc[s, t] = series[col]\n",
    "        return mat.astype(float)\n",
    "    \n",
    "    z_mat = to_matrix(z_data)\n",
    "    res_mat = to_matrix(res_data)\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(24, 10))\n",
    "    sns.heatmap(z_mat, ax=ax1, cmap=\"RdBu_r\", center=0, annot=False, cbar_kws={'label': 'Sigma'})\n",
    "    ax1.set_title(f\"Z-Score (Statistical Rich/Cheap)\\nDate: {target_date}\", fontsize=14)\n",
    "    sns.heatmap(res_mat, ax=ax2, cmap=\"RdBu_r\", center=0, annot=False, cbar_kws={'label': 'Basis Points'})\n",
    "    ax2.set_title(f\"PCA Residuals (Model Rich/Cheap)\\nBlue=Rich, Red=Cheap\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "dates_list = df_fwd.index.tolist()\n",
    "date_slider = widgets.SelectionSlider(\n",
    "    options=[(d.strftime('%Y-%m-%d'), i) for i, d in enumerate(dates_list)],\n",
    "    value=len(dates_list)-1,\n",
    "    description='Date:',\n",
    "    layout=widgets.Layout(width='80%')\n",
    ")\n",
    "\n",
    "interact(show_dashboard, date_idx=date_slider);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": { "display_name": "Python 3", "language": "python", "name": "python3" },
  "language_info": { "name": "python", "version": "3.12.3" },
  "nbformat": 4, "nbformat_minor": 2
 }
}
