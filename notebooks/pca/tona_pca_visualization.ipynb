{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c18d7b01",
   "metadata": {},
   "source": [
    "# TONA OIS PCA Visualization Tool (2D Surface Analysis)\n",
    "\n",
    "This notebook analyzes the TONA OIS market using PCA on the **Forward Rate Surface**.\n",
    "We construct a matrix of Forward Rates:\n",
    "- **Start Years (Horizon):** 1Y to 10Y (Forward Start)\n",
    "- **Tenors (Duration):** 1Y to 40Y (Underlying Rate)\n",
    "\n",
    "This results in a 40x10 grid (400 points) per date. We analyze the principal components of this surface and visualize the residuals (Rich/Cheap) as a 2D Heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725dfde9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import QuantLib as ql\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import ipywidgets as widgets\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "%matplotlib inline\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316a2d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Data Configuration ---\n",
    "DATA_DIR = \"../../external_data\"\n",
    "FILES = [\"1-9.xlsx\", \"10-40 (1).xlsx\"]\n",
    "MIN_YEAR = 1\n",
    "\n",
    "# Configuration for Analysis Grid\n",
    "MAX_START_YEAR = 10  # \"10年先まで\"\n",
    "MAX_TENOR_YEAR = 40  # \"40年(のデータ)\"\n",
    "\n",
    "def parse_japanese_date(date_str):\n",
    "    try:\n",
    "        match = re.match(r'(\\d{4})年(\\d{1,2})月(\\d{1,2})日', str(date_str))\n",
    "        if match:\n",
    "            return datetime(int(match.group(1)), int(match.group(2)), int(match.group(3)))\n",
    "    except:\n",
    "        pass\n",
    "    return None\n",
    "\n",
    "def load_and_merge_data():\n",
    "    dfs = []\n",
    "    for f in FILES:\n",
    "        path = os.path.join(DATA_DIR, f)\n",
    "        if not os.path.exists(path):\n",
    "            # Fallback paths\n",
    "            path = os.path.abspath(os.path.join(os.getcwd(), \"../../external_data\", f))\n",
    "            if not os.path.exists(path):\n",
    "                 path = os.path.abspath(os.path.join(os.getcwd(), \"../../external_data\", f))\n",
    "        \n",
    "        try:\n",
    "            df = pd.read_excel(path)\n",
    "            df = df.drop(0)\n",
    "            df['Date'] = df['日付'].apply(parse_japanese_date)\n",
    "            df = df.dropna(subset=['Date'])\n",
    "            dfs.append(df)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {f}: {e}\")\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    if not dfs: return pd.DataFrame()\n",
    "\n",
    "    full_df = dfs[0]\n",
    "    for i in range(1, len(dfs)):\n",
    "        full_df = pd.merge(full_df, dfs[i], on='Date', how='inner', suffixes=('', '_drop'))\n",
    "        cols_to_drop = [c for c in full_df.columns if '_drop' in c or c == '日付_drop']\n",
    "        full_df = full_df.drop(columns=cols_to_drop)\n",
    "        \n",
    "    tenor_map = {}\n",
    "    for col in full_df.columns:\n",
    "        match = re.search(r'JPYTOCOIS(\\d+)Y=TRDJ \\(BID\\)', col)\n",
    "        if match:\n",
    "            year = int(match.group(1))\n",
    "            if year >= MIN_YEAR:\n",
    "                tenor_map[year] = col\n",
    "                \n",
    "    cols = ['Date'] + list(tenor_map.values())\n",
    "    final_df = full_df[cols].copy()\n",
    "    final_df = final_df.rename(columns={v: k for k, v in tenor_map.items()})\n",
    "    final_df = final_df.sort_values('Date').reset_index(drop=True)\n",
    "    \n",
    "    for y in tenor_map.keys():\n",
    "        final_df[y] = pd.to_numeric(final_df[y], errors='coerce')\n",
    "        \n",
    "    final_df = final_df.dropna()\n",
    "    return final_df\n",
    "\n",
    "def calculate_forward_matrix(df):\n",
    "    \"\"\"\n",
    "    Constructs QL curve and calculates a matrix of Forward Rates.\n",
    "    Start: 1..MAX_START_YEAR\n",
    "    Tenor: 1..MAX_TENOR_YEAR\n",
    "    \"\"\"\n",
    "    calendar = ql.Japan()\n",
    "    day_count = ql.Actual365Fixed()\n",
    "    tona = ql.OvernightIndex(\"TONA\", 0, ql.JPYCurrency(), calendar, day_count)\n",
    "    \n",
    "    forward_results = []\n",
    "    dates = []\n",
    "    \n",
    "    available_tenors = [c for c in df.columns if isinstance(c, int)]\n",
    "    available_tenors.sort()\n",
    "    \n",
    "    print(f\"Calculating {MAX_START_YEAR}x{MAX_TENOR_YEAR} Forward Matrix for {len(df)} days...\")\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        eval_date = row['Date']\n",
    "        ql_eval_date = ql.Date(eval_date.day, eval_date.month, eval_date.year)\n",
    "        ql.Settings.instance().evaluationDate = ql_eval_date\n",
    "        \n",
    "        helpers = []\n",
    "        for t in available_tenors:\n",
    "            rate = row[t]\n",
    "            if np.isnan(rate): continue\n",
    "            quote = ql.SimpleQuote(rate / 100.0)\n",
    "            helper = ql.OISRateHelper(2, ql.Period(t, ql.Years), ql.QuoteHandle(quote), tona)\n",
    "            helpers.append(helper)\n",
    "            \n",
    "        try:\n",
    "            curve = ql.PiecewiseLogCubicDiscount(0, calendar, helpers, day_count)\n",
    "            curve.enableExtrapolation() # Needed for Start+Tenor > 40\n",
    "            \n",
    "            row_data = {}\n",
    "            \n",
    "            # Calculate Grid\n",
    "            for start in range(1, MAX_START_YEAR + 1):\n",
    "                for tenor in range(1, MAX_TENOR_YEAR + 1):\n",
    "                    # For Tenor=1, we can use simple forward rate.\n",
    "                    # For Tenor>1, ideally Par Swap Rate.\n",
    "                    # Approximation: (DF_start - DF_end) / Annuity\n",
    "                    # Given \"accurately draw fwd curve\", we should be careful.\n",
    "                    # However, calling MakeVanillaSwap is slow.\n",
    "                    # We will use the proper forwardRate method from QuantLib which computes \n",
    "                    # simple compounding rate for the given period. \n",
    "                    # For long tenors, this is NOT the Swap Rate, it's the simple rate.\n",
    "                    # BUT, \"Forward Rate\" in loose context often means the Swap Rate for that tenor.\n",
    "                    # Let's assume implied Swap Rate.\n",
    "                    \n",
    "                    d_start = calendar.advance(ql_eval_date, ql.Period(start, ql.Years))\n",
    "                    d_end = calendar.advance(d_start, ql.Period(tenor, ql.Years))\n",
    "                    \n",
    "                    # Calculate Par Rate approximately using discount factors to be fast\n",
    "                    # Swap Rate = (DF(Start) - DF(End)) / Sum(DF(PaymentDates))\n",
    "                    # Assuming annual payments for simplicity and speed\n",
    "                    \n",
    "                    df_start = curve.discount(d_start)\n",
    "                    df_end = curve.discount(d_end)\n",
    "                    \n",
    "                    # Annuity approximation (annual steps)\n",
    "                    annuity = 0.0\n",
    "                    for k in range(1, tenor + 1):\n",
    "                        d_pay = calendar.advance(d_start, ql.Period(k, ql.Years))\n",
    "                        annuity += curve.discount(d_pay)\n",
    "                        \n",
    "                    if annuity == 0:\n",
    "                        swap_rate = 0.0\n",
    "                    else:\n",
    "                        swap_rate = (df_start - df_end) / annuity\n",
    "                        \n",
    "                    # Store as \"S{start}_T{tenor}\"\n",
    "                    col_name = f\"S{start}_T{tenor}\"\n",
    "                    row_data[col_name] = swap_rate * 100.0\n",
    "            \n",
    "            dates.append(eval_date)\n",
    "            forward_results.append(row_data)\n",
    "            \n",
    "        except RuntimeError:\n",
    "            continue\n",
    "\n",
    "    fwd_df = pd.DataFrame(forward_results)\n",
    "    fwd_df['Date'] = dates\n",
    "    fwd_df = fwd_df.set_index('Date')\n",
    "    return fwd_df\n",
    "\n",
    "# Load and Process Data\n",
    "raw_df = load_and_merge_data()\n",
    "if raw_df.empty:\n",
    "    print(\"No data loaded.\")\n",
    "else:\n",
    "    full_fwd_df = calculate_forward_matrix(raw_df)\n",
    "    print(f\"Data Ready: {len(full_fwd_df)} days. Columns: {len(full_fwd_df.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79c52478",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_interactive_pca(pcs_to_use):\n",
    "    data = full_fwd_df.dropna()\n",
    "    if data.empty:\n",
    "        print(\"No valid data.\")\n",
    "        return\n",
    "\n",
    "    # Standardize\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(data)\n",
    "    \n",
    "    # PCA\n",
    "    n_components = min(50, len(data)) # Allow more PCs since we have 400 features\n",
    "    pca = PCA(n_components=n_components)\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Reconstruct\n",
    "    n_use = min(pcs_to_use, n_components)\n",
    "    pca_subset = PCA(n_components=n_use)\n",
    "    X_subset = pca_subset.fit_transform(X_scaled)\n",
    "    X_recon_scaled = pca_subset.inverse_transform(X_subset)\n",
    "    X_recon = scaler.inverse_transform(X_recon_scaled)\n",
    "    \n",
    "    recon_df = pd.DataFrame(X_recon, index=data.index, columns=data.columns)\n",
    "    residuals = data - recon_df\n",
    "    \n",
    "    # Latest Analysis\n",
    "    latest_date = data.index[-1]\n",
    "    latest_resid = residuals.loc[latest_date]\n",
    "    \n",
    "    # --- Transform 1D Residuals back to 2D Matrix for Heatmap ---\n",
    "    # Our columns are \"S{start}_T{tenor}\"\n",
    "    # Start: 1..10 (Rows of Heatmap? Or Cols?)\n",
    "    # Tenor: 1..40 (Cols of Heatmap? Or Rows?)\n",
    "    # Let's put Start Year on X-axis (1..10) and Tenor on Y-axis (1..40)\n",
    "    \n",
    "    matrix = np.zeros((40, 10)) # Rows=Tenor(40), Cols=Start(10)\n",
    "    \n",
    "    for start in range(1, 11):\n",
    "        for tenor in range(1, 41):\n",
    "            col_name = f\"S{start}_T{tenor}\"\n",
    "            if col_name in latest_resid:\n",
    "                # Array indices are 0-based\n",
    "                matrix[tenor-1, start-1] = latest_resid[col_name]\n",
    "                \n",
    "    # Visualization\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    sns.heatmap(matrix, cmap=\"RdYlGn_r\", center=0,\n",
    "                xticklabels=range(1, 11), \n",
    "                yticklabels=range(1, 41),\n",
    "                cbar_kws={'label': 'Residual (bps)'})\n",
    "                \n",
    "    plt.title(f\"TONA Forward Swap Residuals (Actual - Model) on {latest_date.strftime('%Y-%m-%d')}\n",
    "              f\"Model: PCA 1-{n_use} | X: Start Year (1-10) | Y: Tenor (1-40)\")\n",
    "    plt.xlabel(\"Forward Start Year (In X Years)\")\n",
    "    plt.ylabel(\"Swap Tenor (Duration)\")\n",
    "    plt.gca().invert_yaxis() # Usually short tenor at bottom? No, heatmap standard is top-down.\n",
    "    # Let's keep Tenor 1 at top or bottom?\n",
    "    # Standard matrix: index 0 is top. So Tenor 1 is top.\n",
    "    # If we want Tenor 1 at bottom, invert.\n",
    "    plt.gca().invert_yaxis()\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Stats\n",
    "    print(f\"Explained Variance (first {n_use} PCs): {np.sum(pca.explained_variance_ratio_[:n_use]):.2%}\")\n",
    "    \n",
    "    # Find max mispricing\n",
    "    cheapest = latest_resid.idxmax()\n",
    "    richest = latest_resid.idxmin()\n",
    "    print(f\"Cheapest Point: {cheapest} ({latest_resid.max():.2f} bps)\")\n",
    "    print(f\"Richest Point:  {richest} ({latest_resid.min():.2f} bps)\")\n",
    "\n",
    "# Create Widgets\n",
    "style = {'description_width': 'initial'}\n",
    "w_pcs = widgets.IntSlider(value=3, min=1, max=10, step=1, description='PCs to Use:', style=style)\n",
    "\n",
    "ui = widgets.VBox([w_pcs])\n",
    "out = widgets.interactive_output(run_interactive_pca, {'pcs_to_use': w_pcs})\n",
    "\n",
    "display(ui, out)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
