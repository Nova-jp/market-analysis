#!/usr/bin/env python3
"""
JSDA Parser - JSDAã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime, date
from typing import List, Set
import logging

class JSDAParser:
    """JSDAã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’è§£æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self):
        self.base_url = "https://market.jsda.or.jp"
        self.index_url = "https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/index.html"
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def get_available_dates_from_html(self) -> List[date]:
        """
        JSDAã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ã‚’å–å¾—
        ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ + ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ï¼ˆ2002å¹´ã€œç¾åœ¨ï¼‰ã‹ã‚‰å…¨æ—¥ä»˜ã‚’å–å¾—
        """
        try:
            self.logger.info("ğŸ“¡ JSDAã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’å–å¾—ä¸­...")
            
            available_dates = set()
            today = date.today()
            current_year = today.year
            
            # 1. ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—
            self.logger.info("ğŸ“„ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°æ—¥ä»˜ã‚’å–å¾—...")
            main_dates = self._parse_main_page()
            available_dates.update(main_dates)
            
            # 2. ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰å„å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ï¼ˆ2002å¹´ã€œç¾åœ¨ã¾ã§ï¼‰
            # ã‚ˆã‚Šå¤šãã®éå»ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ãŸã‚2002å¹´ã‹ã‚‰é–‹å§‹
            start_year = 2002  
            for year in range(start_year, current_year + 1):
                self.logger.info(f"ğŸ“„ {year}å¹´ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
                archive_dates = self._parse_archive_page(year)
                available_dates.update(archive_dates)
                
                # JSDAæ¥ç¶šåˆ¶é™å›é¿ã®ãŸã‚20ç§’å¾…æ©Ÿï¼ˆãƒ¦ãƒ¼ã‚¶ãƒ¼è¦æ±‚ï¼‰
                if year < current_year:  # æœ€å¾Œã®å¹´ä»¥å¤–
                    self.logger.info(f"  â±ï¸  20ç§’å¾…æ©Ÿä¸­...")
                    time.sleep(20)
            
            if len(available_dates) > 0:
                # æ—¥ä»˜ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆä»Šæ—¥ã‹ã‚‰æ˜”ã¸ï¼‰
                sorted_dates = sorted(list(available_dates), reverse=True)
                
                # ç¾åœ¨æ—¥ã‚ˆã‚Šæœªæ¥ã®æ—¥ä»˜ã‚’é™¤å¤–
                today = date.today()
                filtered_dates = [d for d in sorted_dates if d <= today]
                
                self.logger.info(f"âœ… JSDA HTMLè§£ææˆåŠŸ: {len(filtered_dates)}æ—¥åˆ†")
                if filtered_dates:
                    self.logger.info(f"ğŸ“… æœ€æ–°: {filtered_dates[0]} ï½ æœ€å¤: {filtered_dates[-1]}")
                
                return filtered_dates
            
        except Exception as e:
            self.logger.error(f"JSDAã‚µã‚¤ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        # HTMLã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã€ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªã«ã‚ˆã‚‹å–å¾—ã‚’è©¦è¡Œ
        return self._get_dates_by_file_checking()
    
    def _get_dates_by_file_checking(self) -> List[date]:
        """
        JSDAã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«åŸºã¥ã„ã¦å­˜åœ¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ã‚’ç¢ºèª
        2002å¹´ã‹ã‚‰ç¾åœ¨ã¾ã§ç³»çµ±çš„ã«ãƒã‚§ãƒƒã‚¯
        """
        self.logger.info("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªã«ã‚ˆã‚‹æ—¥ä»˜å–å¾—ã‚’é–‹å§‹...")
        
        available_dates = set()
        today = date.today()
        
        # å¹´åˆ¥ã«ãƒã‚§ãƒƒã‚¯ï¼ˆ2002å¹´ã‹ã‚‰ç¾åœ¨ã¾ã§ï¼‰
        for year in range(2002, today.year + 1):
            year_dates = 0
            
            # æœˆåˆ¥ãƒã‚§ãƒƒã‚¯
            for month in range(1, 13):
                if year == today.year and month > today.month:
                    break
                
                # å„æœˆã®å–¶æ¥­æ—¥ã‚’ãƒã‚§ãƒƒã‚¯
                from calendar import monthrange
                _, days_in_month = monthrange(year, month)
                
                for day in range(1, days_in_month + 1):
                    if year == today.year and month == today.month and day > today.day:
                        break
                    
                    check_date = date(year, month, day)
                    
                    # å–¶æ¥­æ—¥ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆåœŸæ—¥ã‚’é™¤å¤–ï¼‰
                    if not self.is_business_day(check_date):
                        continue
                    
                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                    if self._check_csv_file_exists(check_date):
                        available_dates.add(check_date)
                        year_dates += 1
                        
                        # JSDAä¿è­·ã®ãŸã‚20ç§’é–“éš”ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
                        time.sleep(20)
                    
                    # é€²è¡ŒçŠ¶æ³è¡¨ç¤ºï¼ˆ100æ—¥ã”ã¨ï¼‰
                    if len(available_dates) % 100 == 0 and len(available_dates) > 0:
                        self.logger.info(f"  ğŸ“Š {len(available_dates)}æ—¥åˆ†ç¢ºèªæ¸ˆã¿...")
            
            if year_dates > 0:
                self.logger.info(f"  âœ… {year}å¹´: {year_dates}æ—¥åˆ†")
            elif year >= 2010:  # 2010å¹´ä»¥é™ã§ãƒ‡ãƒ¼ã‚¿ãŒ0æ—¥ã¯ç•°å¸¸
                self.logger.warning(f"  âš ï¸ {year}å¹´: ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # æ—¥ä»˜ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆä»Šæ—¥ã‹ã‚‰æ˜”ã¸ï¼‰
        sorted_dates = sorted(list(available_dates), reverse=True)
        
        self.logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªå®Œäº†: {len(sorted_dates)}æ—¥åˆ†")
        if sorted_dates:
            self.logger.info(f"ğŸ“… æœ€æ–°: {sorted_dates[0]} ï½ æœ€å¤: {sorted_dates[-1]}")
        
        return sorted_dates
    
    def _check_csv_file_exists(self, check_date: date) -> bool:
        """
        ç‰¹å®šæ—¥ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        2019å¹´ä»¥å‰: /files/YYYY/MM/S{YYMMDD}.csv
        2020å¹´ä»¥é™: /files/YYYY/S{YYMMDD}.csv
        """
        year = check_date.year
        month = check_date.month
        date_str = check_date.strftime("%y%m%d")
        
        # 2019å¹´ä»¥å‰ã¯æœˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        if year <= 2019:
            url = f"https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/files/{year}/{month:02d}/S{date_str}.csv"
        else:
            # 2020å¹´ä»¥é™ã¯å¹´åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿
            url = f"https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/files/{year}/S{date_str}.csv"
        
        try:
            response = requests.head(url, timeout=15)
            return response.status_code == 200
        except:
            return False
    
    def _generate_fallback_dates(self) -> List[date]:
        """
        JSDAã‚µã‚¤ãƒˆã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        ç›´è¿‘ã®å–¶æ¥­æ—¥ã‚’ç”Ÿæˆ
        """
        self.logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´è¿‘å–¶æ¥­æ—¥ã‚’ç”Ÿæˆä¸­...")
        
        today = date.today()
        dates = []
        current = today
        
        # ç›´è¿‘3å¹´åˆ†ã®å–¶æ¥­æ—¥ã‚’ç”Ÿæˆ
        for _ in range(800):  # ç´„3å¹´åˆ†
            # å¹³æ—¥ã®ã¿è¿½åŠ ï¼ˆåœŸæ—¥ã‚’é™¤å¤–ï¼‰
            if current.weekday() < 5:
                dates.append(current)
            
            # 1æ—¥å‰ã«ç§»å‹•
            from datetime import timedelta
            current = current - timedelta(days=1)
            
            if current.year < 2015:  # 2015å¹´ã‚ˆã‚Šå¤ã„ãƒ‡ãƒ¼ã‚¿ã¯å¯¾è±¡å¤–
                break
        
        self.logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ—¥ä»˜ç”Ÿæˆå®Œäº†: {len(dates)}æ—¥åˆ†")
        return dates
    
    def is_business_day(self, check_date: date) -> bool:
        """
        å–¶æ¥­æ—¥ã‹ã©ã†ã‹ã‚’ç°¡æ˜“åˆ¤å®šï¼ˆåœŸæ—¥ã‚’é™¤å¤–ï¼‰
        """
        return check_date.weekday() < 5
    
    def _parse_main_page(self) -> Set[date]:
        """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—"""
        available_dates = set()
        
        try:
            response = requests.get(self.index_url, timeout=15)
            response.raise_for_status()
            
            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and ('S' in href and '.csv' in href):
                    match = re.search(r'S(\d{6})\.csv', href)
                    if match:
                        date_str = match.group(1)
                        try:
                            if len(date_str) == 6:
                                year = int('20' + date_str[:2])
                                month = int(date_str[2:4])
                                day = int(date_str[4:6])
                                parsed_date = date(year, month, day)
                                available_dates.add(parsed_date)
                        except ValueError:
                            continue
            
            self.logger.info(f"  ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸: {len(available_dates)}æ—¥åˆ†")
            
        except Exception as e:
            self.logger.warning(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return available_dates
    
    def _parse_archive_page(self, year: int) -> Set[date]:
        """æŒ‡å®šå¹´ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãï¼‰"""
        available_dates = set()
        archive_url = f"{self.base_url}/shijyo/saiken/baibai/baisanchi/archive{year}.html"
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        max_retries = 3
        timeout_seconds = 45  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’45ç§’ã«å»¶é•·
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"  ğŸ“¡ {year}å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                
                response = requests.get(archive_url, timeout=timeout_seconds)
                response.raise_for_status()
                
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—çµ‚äº†
                
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 30  # 30ç§’ã€60ç§’ã€90ç§’å¾…æ©Ÿ
                    self.logger.warning(f"  âš ï¸  {year}å¹´: æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}), {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.warning(f"  âŒ {year}å¹´: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤æ•°åˆ°é”ã€ã‚¹ã‚­ãƒƒãƒ—")
                    return available_dates
        
        try:
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and ('S' in href and '.csv' in href):
                    match = re.search(r'S(\d{6})\.csv', href)
                    if match:
                        date_str = match.group(1)
                        try:
                            if len(date_str) == 6:
                                parsed_year = int('20' + date_str[:2])
                                month = int(date_str[2:4])
                                day = int(date_str[4:6])
                                
                                # æŒ‡å®šå¹´ã¨ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
                                if parsed_year == year:
                                    parsed_date = date(parsed_year, month, day)
                                    available_dates.add(parsed_date)
                        except ValueError:
                            continue
            
            self.logger.info(f"  {year}å¹´: {len(available_dates)}æ—¥åˆ†")
            
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                self.logger.info(f"  {year}å¹´: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãªã—")
            else:
                self.logger.warning(f"  {year}å¹´ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return available_dates
