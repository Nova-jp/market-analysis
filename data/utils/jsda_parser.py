#!/usr/bin/env python3
"""
JSDA Parser - JSDAã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’å–å¾—
"""

import requests
from bs4 import BeautifulSoup
import re
import time
from datetime import datetime, date
from typing import List, Set, Optional, Dict
import logging
import json
import os
from pathlib import Path

class JSDAParser:
    """JSDAã‚µã‚¤ãƒˆã‹ã‚‰ãƒ‡ãƒ¼ã‚¿åˆ©ç”¨å¯èƒ½æ—¥ä»˜ã‚’è§£æã™ã‚‹ã‚¯ãƒ©ã‚¹"""
    
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹ï¼ˆãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆã‹ã‚‰ã®ç›¸å¯¾ãƒ‘ã‚¹ï¼‰
    CACHE_FILE_PATH = "data_files/jsda_available_dates.json"
    
    def __init__(self):
        self.base_url = "https://market.jsda.or.jp"
        self.index_url = "https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/index.html"
        
        logging.basicConfig(level=logging.INFO)
        self.logger = logging.getLogger(__name__)
    
    def _get_cache_path(self) -> Path:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã®çµ¶å¯¾ãƒ‘ã‚¹ã‚’å–å¾—"""
        # ã“ã®ãƒ•ã‚¡ã‚¤ãƒ«(data/utils/jsda_parser.py)ã‹ã‚‰ãƒ—ãƒ­ã‚¸ã‚§ã‚¯ãƒˆãƒ«ãƒ¼ãƒˆ(../../)ã¸
        current_dir = Path(__file__).resolve().parent
        project_root = current_dir.parent.parent
        return project_root / self.CACHE_FILE_PATH

    def _load_cache(self) -> Optional[List[date]]:
        """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’èª­ã¿è¾¼ã‚€"""
        cache_path = self._get_cache_path()
        if not cache_path.exists():
            return None
        
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
                
            dates_str = data.get('dates', [])
            # æ–‡å­—åˆ— -> dateã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆ
            dates = [date.fromisoformat(d) for d in dates_str]
            self.logger.info(f"ğŸ“‚ ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿æˆåŠŸ: {len(dates)}æ—¥åˆ† (æ›´æ–°: {data.get('last_updated')})")
            return dates
        except Exception as e:
            self.logger.warning(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥èª­ã¿è¾¼ã¿ã‚¨ãƒ©ãƒ¼: {e}")
            return None

    def _save_cache(self, dates: List[date]):
        """æ—¥ä»˜ãƒªã‚¹ãƒˆã‚’ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        cache_path = self._get_cache_path()
        try:
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            
            # æ—¥ä»˜ã‚’æ–‡å­—åˆ—ã«å¤‰æ›ã—ã¦é™é †ã‚½ãƒ¼ãƒˆ
            sorted_dates = sorted(list(set(dates)), reverse=True)
            dates_str = [d.isoformat() for d in sorted_dates]
            
            data = {
                "last_updated": datetime.now().isoformat(),
                "dates": dates_str
            }
            
            with open(cache_path, 'w', encoding='utf-8') as f:
                json.dump(data, f, indent=2, ensure_ascii=False)
                
            self.logger.info(f"ğŸ’¾ ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜å®Œäº†: {len(dates_str)}æ—¥åˆ† -> {cache_path}")
        except Exception as e:
            self.logger.error(f"ã‚­ãƒ£ãƒƒã‚·ãƒ¥ä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")

    def get_available_dates_from_html(self, start_year: int = 2002, use_cache: bool = True, force_refresh: bool = False) -> List[date]:
        """
        JSDAã‹ã‚‰åˆ©ç”¨å¯èƒ½ãªãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ã‚’å–å¾—
        ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ + ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—
        Args:
            start_year: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–å–å¾—é–‹å§‹å¹´ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2002ï¼‰
            use_cache: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã™ã‚‹ã‹ã©ã†ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Trueï¼‰
            force_refresh: ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ç„¡è¦–ã—ã¦å…¨ä»¶å–å¾—ã™ã‚‹ã‹ï¼ˆãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: Falseï¼‰
        """
        try:
            # 1. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã®ä½¿ç”¨ã‚’è©¦ã¿ã‚‹
            if use_cache and not force_refresh:
                cached_dates = self._load_cache()
                if cached_dates:
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚‹å ´åˆã€ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã ã‘ç¢ºèªã—ã¦å·®åˆ†æ›´æ–°ï¼ˆé«˜é€ŸåŒ–ï¼‰
                    self.logger.info("ğŸ“„ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°æ—¥ä»˜ã®ã¿ç¢ºèªä¸­...")
                    main_dates = self._parse_main_page()
                    
                    # æ–°ã—ã„æ—¥ä»˜ãŒã‚ã‚‹ã‹ç¢ºèª
                    new_dates_found = False
                    current_dates_set = set(cached_dates)
                    
                    for d in main_dates:
                        if d not in current_dates_set:
                            self.logger.info(f"ğŸ†• æ–°ã—ã„æ—¥ä»˜ã‚’ç™ºè¦‹: {d}")
                            current_dates_set.add(d)
                            new_dates_found = True
                    
                    # æ›´æ–°ãŒã‚ã‚Œã°ä¿å­˜
                    if new_dates_found:
                        updated_dates = list(current_dates_set)
                        self._save_cache(updated_dates)
                        return sorted(updated_dates, reverse=True)
                    else:
                        self.logger.info("âœ… ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã¯æœ€æ–°ã§ã™")
                        return cached_dates

            # 2. ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒãªã„ã€ã¾ãŸã¯å¼·åˆ¶æ›´æ–°ã®å ´åˆã¯å…¨å–å¾—
            self.logger.info("ğŸ“¡ JSDAã‚µã‚¤ãƒˆã‹ã‚‰å…¨æœŸé–“ã®ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ã‚’å–å¾—ä¸­...")
            
            available_dates = set()
            today = date.today()
            current_year = today.year
            
            # ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—
            self.logger.info("ğŸ“„ ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°æ—¥ä»˜ã‚’å–å¾—...")
            main_dates = self._parse_main_page()
            available_dates.update(main_dates)
            
            # ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰å„å¹´ã®ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
            real_start_year = max(start_year, 2002)
            
            for year in range(real_start_year, current_year + 1):
                self.logger.info(f"ğŸ“„ {year}å¹´ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­...")
                archive_dates = self._parse_archive_page(year)
                available_dates.update(archive_dates)
                
                # JSDAæ¥ç¶šåˆ¶é™å›é¿ã®ãŸã‚å¾…æ©Ÿ
                if year < current_year and real_start_year < current_year:
                    self.logger.info(f"  â±ï¸  30ç§’å¾…æ©Ÿä¸­...")
                    time.sleep(30)
            
            if len(available_dates) > 0:
                # æ—¥ä»˜ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆ
                sorted_dates = sorted(list(available_dates), reverse=True)
                
                # ç¾åœ¨æ—¥ã‚ˆã‚Šæœªæ¥ã®æ—¥ä»˜ã‚’é™¤å¤–
                filtered_dates = [d for d in sorted_dates if d <= today]
                
                self.logger.info(f"âœ… JSDA HTMLè§£ææˆåŠŸ: {len(filtered_dates)}æ—¥åˆ†")
                if filtered_dates:
                    self.logger.info(f"ğŸ“… æœ€æ–°: {filtered_dates[0]} ï½ æœ€å¤: {filtered_dates[-1]}")
                    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
                    self._save_cache(filtered_dates)
                
                return filtered_dates
            
        except Exception as e:
            self.logger.error(f"JSDAã‚µã‚¤ãƒˆè§£æã‚¨ãƒ©ãƒ¼: {e}")
            # ã‚¨ãƒ©ãƒ¼æ™‚ã§ã‚‚ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒã‚ã‚Œã°ãã‚Œã‚’è¿”ã™
            if use_cache:
                cached = self._load_cache()
                if cached:
                    self.logger.info("âš ï¸ ã‚¨ãƒ©ãƒ¼ç™ºç”Ÿã®ãŸã‚ã€æ—¢å­˜ã®ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨ã—ã¾ã™")
                    return cached
        
        # HTMLã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆ
        if start_year <= 2020:
             return self._get_dates_by_file_checking()
        else:
             return []
    
    def _get_dates_by_file_checking(self) -> List[date]:
        """
        JSDAã®ãƒ•ã‚¡ã‚¤ãƒ«æ§‹é€ ã«åŸºã¥ã„ã¦å­˜åœ¨ã™ã‚‹ãƒ‡ãƒ¼ã‚¿æ—¥ä»˜ã‚’ç¢ºèª
        2002å¹´ã‹ã‚‰ç¾åœ¨ã¾ã§ç³»çµ±çš„ã«ãƒã‚§ãƒƒã‚¯
        """
        self.logger.info("ğŸ” ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªã«ã‚ˆã‚‹æ—¥ä»˜å–å¾—ã‚’é–‹å§‹...")
        
        available_dates = set()
        today = date.today()
        
        # å¹´åˆ¥ã«ãƒã‚§ãƒƒã‚¯ï¼ˆ2002å¹´ã‹ã‚‰ç¾åœ¨ã¾ã§ï¼‰
        for year in range(2002, today.year + 1):
            year_dates = 0
            
            # æœˆåˆ¥ãƒã‚§ãƒƒã‚¯
            for month in range(1, 13):
                if year == today.year and month > today.month:
                    break
                
                # å„æœˆã®å–¶æ¥­æ—¥ã‚’ãƒã‚§ãƒƒã‚¯
                from calendar import monthrange
                _, days_in_month = monthrange(year, month)
                
                for day in range(1, days_in_month + 1):
                    if year == today.year and month == today.month and day > today.day:
                        break
                    
                    check_date = date(year, month, day)
                    
                    # å–¶æ¥­æ—¥ã®ã¿ãƒã‚§ãƒƒã‚¯ï¼ˆåœŸæ—¥ã‚’é™¤å¤–ï¼‰
                    if not self.is_business_day(check_date):
                        continue
                    
                    # CSVãƒ•ã‚¡ã‚¤ãƒ«ã®å­˜åœ¨ç¢ºèª
                    if self._check_csv_file_exists(check_date):
                        available_dates.add(check_date)
                        year_dates += 1
                        
                        # JSDAä¿è­·ã®ãŸã‚20ç§’é–“éš”ã§ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèª
                        time.sleep(20)
                    
                    # é€²è¡ŒçŠ¶æ³è¡¨ç¤ºï¼ˆ100æ—¥ã”ã¨ï¼‰
                    if len(available_dates) % 100 == 0 and len(available_dates) > 0:
                        self.logger.info(f"  ğŸ“Š {len(available_dates)}æ—¥åˆ†ç¢ºèªæ¸ˆã¿...")
            
            if year_dates > 0:
                self.logger.info(f"  âœ… {year}å¹´: {year_dates}æ—¥åˆ†")
            elif year >= 2010:  # 2010å¹´ä»¥é™ã§ãƒ‡ãƒ¼ã‚¿ãŒ0æ—¥ã¯ç•°å¸¸
                self.logger.warning(f"  âš ï¸ {year}å¹´: ãƒ‡ãƒ¼ã‚¿ãªã—")
        
        # æ—¥ä»˜ã‚’é™é †ã§ã‚½ãƒ¼ãƒˆï¼ˆä»Šæ—¥ã‹ã‚‰æ˜”ã¸ï¼‰
        sorted_dates = sorted(list(available_dates), reverse=True)
        
        self.logger.info(f"âœ… ãƒ•ã‚¡ã‚¤ãƒ«å­˜åœ¨ç¢ºèªå®Œäº†: {len(sorted_dates)}æ—¥åˆ†")
        if sorted_dates:
            self.logger.info(f"ğŸ“… æœ€æ–°: {sorted_dates[0]} ï½ æœ€å¤: {sorted_dates[-1]}")
        
        return sorted_dates
    
    def _check_csv_file_exists(self, check_date: date) -> bool:
        """
        ç‰¹å®šæ—¥ã®CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯
        2019å¹´ä»¥å‰: /files/YYYY/MM/S{YYMMDD}.csv
        2020å¹´ä»¥é™: /files/YYYY/S{YYMMDD}.csv
        """
        year = check_date.year
        month = check_date.month
        date_str = check_date.strftime("%y%m%d")
        
        # 2019å¹´ä»¥å‰ã¯æœˆåˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 
        if year <= 2019:
            url = f"https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/files/{year}/{month:02d}/S{date_str}.csv"
        else:
            # 2020å¹´ä»¥é™ã¯å¹´åˆ¥ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ã¿
            url = f"https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/files/{year}/S{date_str}.csv"
        
        try:
            response = requests.head(url, timeout=15)
            return response.status_code == 200
        except:
            return False
    
    def _generate_fallback_dates(self) -> List[date]:
        """
        JSDAã‚µã‚¤ãƒˆã‹ã‚‰ã®å–å¾—ã«å¤±æ•—ã—ãŸå ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        ç›´è¿‘ã®å–¶æ¥­æ—¥ã‚’ç”Ÿæˆ
        """
        self.logger.info("ğŸ”„ ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯: ç›´è¿‘å–¶æ¥­æ—¥ã‚’ç”Ÿæˆä¸­...")
        
        today = date.today()
        dates = []
        current = today
        
        # ç›´è¿‘3å¹´åˆ†ã®å–¶æ¥­æ—¥ã‚’ç”Ÿæˆ
        for _ in range(800):  # ç´„3å¹´åˆ†
            # å¹³æ—¥ã®ã¿è¿½åŠ ï¼ˆåœŸæ—¥ã‚’é™¤å¤–ï¼‰
            if current.weekday() < 5:
                dates.append(current)
            
            # 1æ—¥å‰ã«ç§»å‹•
            from datetime import timedelta
            current = current - timedelta(days=1)
            
            if current.year < 2015:  # 2015å¹´ã‚ˆã‚Šå¤ã„ãƒ‡ãƒ¼ã‚¿ã¯å¯¾è±¡å¤–
                break
        
        self.logger.info(f"âœ… ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯æ—¥ä»˜ç”Ÿæˆå®Œäº†: {len(dates)}æ—¥åˆ†")
        return dates
    
    def is_business_day(self, check_date: date) -> bool:
        """
        å–¶æ¥­æ—¥ã‹ã©ã†ã‹ã‚’ç°¡æ˜“åˆ¤å®šï¼ˆåœŸæ—¥ã‚’é™¤å¤–ï¼‰
        """
        return check_date.weekday() < 5
    
    def _parse_main_page(self) -> Set[date]:
        """ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸ã‹ã‚‰æœ€æ–°ã®æ—¥ä»˜ã‚’å–å¾—"""
        available_dates = set()
        
        try:
            response = requests.get(self.index_url, timeout=15)
            response.raise_for_status()
            
            if response.encoding is None or response.encoding == 'ISO-8859-1':
                response.encoding = 'utf-8'
            
            soup = BeautifulSoup(response.text, 'html.parser')
            
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and ('S' in href and '.csv' in href):
                    match = re.search(r'S(\d{6})\.csv', href)
                    if match:
                        date_str = match.group(1)
                        try:
                            if len(date_str) == 6:
                                year = int('20' + date_str[:2])
                                month = int(date_str[2:4])
                                day = int(date_str[4:6])
                                parsed_date = date(year, month, day)
                                available_dates.add(parsed_date)
                        except ValueError:
                            continue
            
            self.logger.info(f"  ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸: {len(available_dates)}æ—¥åˆ†")
            
        except Exception as e:
            self.logger.warning(f"ãƒ¡ã‚¤ãƒ³ãƒšãƒ¼ã‚¸è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return available_dates
    
    def _parse_archive_page(self, year: int) -> Set[date]:
        """æŒ‡å®šå¹´ã®ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ã‹ã‚‰æ—¥ä»˜ã‚’å–å¾—ï¼ˆãƒªãƒˆãƒ©ã‚¤æ©Ÿæ§‹ä»˜ãï¼‰"""
        available_dates = set()
        archive_url = f"{self.base_url}/shijyo/saiken/baibai/baisanchi/archive{year}.html"
        
        # ãƒªãƒˆãƒ©ã‚¤è¨­å®š
        max_retries = 3
        timeout_seconds = 45  # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆã‚’45ç§’ã«å»¶é•·
        
        for attempt in range(max_retries):
            try:
                self.logger.info(f"  ğŸ“¡ {year}å¹´ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­... (è©¦è¡Œ {attempt + 1}/{max_retries})")
                
                response = requests.get(archive_url, timeout=timeout_seconds)
                if response.status_code == 404:
                    self.logger.info(f"  {year}å¹´: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãƒšãƒ¼ã‚¸ãŒã¾ã ã‚ã‚Šã¾ã›ã‚“(404)")
                    return set()
                response.raise_for_status()
                
                if response.encoding is None or response.encoding == 'ISO-8859-1':
                    response.encoding = 'utf-8'
                
                soup = BeautifulSoup(response.text, 'html.parser')
                break  # æˆåŠŸã—ãŸã‚‰ãƒ«ãƒ¼ãƒ—çµ‚äº†
                
            except (requests.exceptions.Timeout, requests.exceptions.ConnectionError) as e:
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 30  # 30ç§’ã€60ç§’ã€90ç§’å¾…æ©Ÿ
                    self.logger.warning(f"  âš ï¸  {year}å¹´: æ¥ç¶šã‚¨ãƒ©ãƒ¼ (è©¦è¡Œ {attempt + 1}), {wait_time}ç§’å¾Œã«ãƒªãƒˆãƒ©ã‚¤...")
                    time.sleep(wait_time)
                    continue
                else:
                    self.logger.warning(f"  âŒ {year}å¹´: æœ€å¤§ãƒªãƒˆãƒ©ã‚¤æ•°åˆ°é”ã€ã‚¹ã‚­ãƒƒãƒ—")
                    return available_dates
        
        try:
            links = soup.find_all('a', href=True)
            for link in links:
                href = link.get('href')
                if href and ('S' in href and '.csv' in href):
                    match = re.search(r'S(\d{6})\.csv', href)
                    if match:
                        date_str = match.group(1)
                        try:
                            if len(date_str) == 6:
                                parsed_year = int('20' + date_str[:2])
                                month = int(date_str[2:4])
                                day = int(date_str[4:6])
                                
                                # æŒ‡å®šå¹´ã¨ä¸€è‡´ã™ã‚‹å ´åˆã®ã¿è¿½åŠ 
                                if parsed_year == year:
                                    parsed_date = date(parsed_year, month, day)
                                    available_dates.add(parsed_date)
                        except ValueError:
                            continue
            
            self.logger.info(f"  {year}å¹´: {len(available_dates)}æ—¥åˆ†")
            
        except Exception as e:
            if "404" in str(e) or "Not Found" in str(e):
                self.logger.info(f"  {year}å¹´: ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ãªã—")
            else:
                self.logger.warning(f"  {year}å¹´ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–è§£æã‚¨ãƒ©ãƒ¼: {e}")
        
        return available_dates
