#!/usr/bin/env python3
"""
æµå‹•æ€§ä¾›çµ¦å…¥æœ­ãƒ‡ãƒ¼ã‚¿åé›†ãƒ¢ã‚¸ãƒ¥ãƒ¼ãƒ«

è²¡å‹™çœã®æµå‹•æ€§ä¾›çµ¦å…¥æœ­ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ï¼ˆExcelï¼‰ã‹ã‚‰:
1. å…¥æœ­ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’å–å¾—
2. Excelå†…ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯ã‹ã‚‰è¿½åŠ ç™ºè¡ŒéŠ˜æŸ„è©³ç´°ã‚’å–å¾—
3. å„éŠ˜æŸ„ã‚’bond_auctionãƒ†ãƒ¼ãƒ–ãƒ«ã«ç™»éŒ²ï¼ˆauction_type='tap'ï¼‰

ãƒ‡ãƒ¼ã‚¿ã‚½ãƒ¼ã‚¹:
- https://www.mof.go.jp/jgbs/reference/appendix/ryudousei_historical_data.xls
"""

import pandas as pd
import xlrd
import requests
from bs4 import BeautifulSoup
from typing import List, Dict, Optional, Any
from datetime import datetime
import logging
import time
import re

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)


class LiquiditySupplyCollector:
    """æµå‹•æ€§ä¾›çµ¦å…¥æœ­ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""

    EXCEL_URL = "https://www.mof.go.jp/jgbs/reference/appendix/ryudousei_historical_data.xls"
    BASE_URL = "https://www.mof.go.jp"

    # å›½ç«‹å›½ä¼šå›³æ›¸é¤¨ WARP ã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®pidå€™è£œï¼ˆæœŸé–“ã”ã¨ã«ç•°ãªã‚‹å¯èƒ½æ€§ï¼‰
    WARP_PIDS = [
        '11949862',  # 2020-2021å¹´é ƒ
        '11424711',  # 2019-2020å¹´é ƒ
        '12654644',  # åˆ¥ã®æœŸé–“å€™è£œ
        '258151',    # åˆ¥ã®æœŸé–“å€™è£œ
        '11618512',  # åˆ¥ã®æœŸé–“å€™è£œ
    ]

    def __init__(self):
        self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36'
        })

    def download_excel(self, save_path: Optional[str] = None) -> str:
        """
        Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰

        Args:
            save_path: ä¿å­˜å…ˆãƒ‘ã‚¹ï¼ˆNoneã®å ´åˆã¯ä¸€æ™‚ãƒ•ã‚¡ã‚¤ãƒ«ï¼‰

        Returns:
            ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ãŸãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        """
        logger.info(f"ğŸ“¥ Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ä¸­: {self.EXCEL_URL}")

        response = self.session.get(self.EXCEL_URL, timeout=30)
        response.raise_for_status()

        if save_path is None:
            import tempfile
            save_path = tempfile.mktemp(suffix='.xls')

        with open(save_path, 'wb') as f:
            f.write(response.content)

        logger.info(f"âœ… ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰å®Œäº†: {save_path}")
        return save_path

    def parse_excel_summary(self, excel_path: str) -> List[Dict[str, Any]]:
        """
        Excelã‹ã‚‰å…¥æœ­ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’æŠ½å‡º

        Args:
            excel_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹

        Returns:
            å…¥æœ­ã‚µãƒãƒªãƒ¼ã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸ“Š Excelãƒ•ã‚¡ã‚¤ãƒ«ã‚’ãƒ‘ãƒ¼ã‚¹ä¸­: {excel_path}")

        # pandasã§èª­ã¿è¾¼ã¿ï¼ˆãƒ‡ãƒ¼ã‚¿æŠ½å‡ºç”¨ï¼‰
        df = pd.read_excel(excel_path, sheet_name='æµå‹•æ€§', header=1)

        # åˆ—åã®æ”¹è¡Œã‚’å‰Šé™¤
        df.columns = df.columns.str.replace('\n', '')

        # ãƒ‡ãƒ¼ã‚¿ã‚¯ãƒ¬ãƒ³ã‚¸ãƒ³ã‚°
        df = df[df['å…¥æœ­æ—¥'].notna()]  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’é™¤å¤–

        summaries = []

        for idx, row in df.iterrows():
            try:
                # å›å·ã‚’æŠ½å‡ºï¼ˆä¾‹: "ç¬¬437å›" â†’ 437ï¼‰
                round_str = str(row.iloc[0])
                round_match = re.search(r'ç¬¬?(\d+)å›?', round_str)
                round_number = int(round_match.group(1)) if round_match else None

                if round_number is None:
                    logger.warning(f"âš ï¸  å›å·ãŒæŠ½å‡ºã§ãã¾ã›ã‚“: {round_str}")
                    continue

                summary = {
                    'round_number': round_number,
                    'auction_date': pd.to_datetime(row['å…¥æœ­æ—¥']).date(),
                    'issue_date': pd.to_datetime(row['ç™ºè¡Œæ—¥']).date(),
                    'planned_amount': self._parse_amount(row['ç™ºè¡Œäºˆå®šé¡']),
                    'offered_amount': self._parse_amount(row['å¿œå‹Ÿé¡']),
                    'allocated_amount': self._parse_amount(row['å‹Ÿå…¥æ±ºå®šé¡']),
                    'highest_yield': self._parse_percentage(row.get('å‹Ÿå…¥æœ€å¤§åˆ©å›æ ¼å·®')),  # æœ€å¤§åˆ©å›æ ¼å·®â†’highest_yield
                    'prorate_ratio': self._parse_percentage(row.get('æ¡ˆåˆ†æ¯”ç‡')),
                    'average_yield': self._parse_percentage(row.get('å‹Ÿå…¥å¹³å‡åˆ©å›æ ¼å·®')),  # å¹³å‡åˆ©å›æ ¼å·®â†’average_yield
                    'row_index': idx + 2,  # pandasã®DataFrame index + header(2è¡Œ)
                    'xlrd_row_index': idx + 2  # xlrdã®å®Ÿéš›ã®è¡Œç•ªå·ï¼ˆ0å§‹ã¾ã‚Š + header 2è¡Œï¼‰
                }

                summaries.append(summary)

            except Exception as e:
                logger.error(f"âŒ è¡Œ {idx} ã®ãƒ‘ãƒ¼ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
                continue

        logger.info(f"âœ… {len(summaries)} ä»¶ã®ã‚µãƒãƒªãƒ¼ã‚’æŠ½å‡º")
        return summaries

    def extract_hyperlink_from_excel(self, excel_path: str, round_number: int, auction_date: Any) -> Optional[str]:
        """
        å…¥æœ­æ—¥ã‹ã‚‰URLã‚’ç”Ÿæˆ

        Args:
            excel_path: Excelãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ï¼ˆæœªä½¿ç”¨ã ãŒäº’æ›æ€§ã®ãŸã‚æ®‹ã™ï¼‰
            round_number: å›å·
            auction_date: å…¥æœ­æ—¥ï¼ˆdateå‹ã¾ãŸã¯datetimeå‹ï¼‰

        Returns:
            ãƒã‚¤ãƒ‘ãƒ¼ãƒªãƒ³ã‚¯URLï¼ˆçµ¶å¯¾ãƒ‘ã‚¹ï¼‰
        """
        try:
            # auction_dateã‚’æ–‡å­—åˆ—ã«å¤‰æ›
            if hasattr(auction_date, 'strftime'):
                date_str = auction_date.strftime('%Y%m%d')
            else:
                # æ–‡å­—åˆ—ã®å ´åˆ
                from datetime import datetime
                date_obj = datetime.strptime(str(auction_date), '%Y-%m-%d')
                date_str = date_obj.strftime('%Y%m%d')

            url = f"{self.BASE_URL}/jgbs/auction/calendar/nyusatsu/resul{date_str}a.htm"
            logger.debug(f"ç¬¬{round_number}å›: {url}")
            return url

        except Exception as e:
            logger.error(f"âŒ URLç”Ÿæˆã‚¨ãƒ©ãƒ¼ (ç¬¬{round_number}å›): {e}")
            return None

    def scrape_issued_bonds(self, url: str) -> List[Dict[str, Any]]:
        """
        è¿½åŠ ç™ºè¡ŒéŠ˜æŸ„ãƒšãƒ¼ã‚¸ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
        404ã‚¨ãƒ©ãƒ¼æ™‚ã¯WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰ã‚‚è©¦è¡Œ

        Args:
            url: è¿½åŠ ç™ºè¡ŒéŠ˜æŸ„ãƒšãƒ¼ã‚¸ã®URL

        Returns:
            ç™ºè¡ŒéŠ˜æŸ„ã®ãƒªã‚¹ãƒˆ
        """
        logger.info(f"ğŸŒ è¿½åŠ ç™ºè¡ŒéŠ˜æŸ„ãƒšãƒ¼ã‚¸ã‚’å–å¾—ä¸­: {url}")

        # ã¾ãšè²¡å‹™çœã®URLã‚’è©¦ã™
        bonds = self._scrape_from_url(url)

        # 404ã‚¨ãƒ©ãƒ¼ã®å ´åˆã€WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‚’è©¦ã™
        if not bonds and '404' in str(getattr(self, '_last_error', '')):
            logger.info(f"ğŸ“š WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰å–å¾—ã‚’è©¦ã¿ã¾ã™")
            bonds = self._scrape_from_warp_archive(url)

        return bonds

    def _scrape_from_url(self, url: str) -> List[Dict[str, Any]]:
        """
        æŒ‡å®šURLã‹ã‚‰éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°

        Args:
            url: ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å¯¾è±¡URL

        Returns:
            ç™ºè¡ŒéŠ˜æŸ„ã®ãƒªã‚¹ãƒˆ
        """
        try:
            time.sleep(2)  # ã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›

            response = self.session.get(url, timeout=30)
            response.raise_for_status()

            # BeautifulSoupã«å…ƒã®ãƒã‚¤ãƒˆãƒ‡ãƒ¼ã‚¿ã‚’æ¸¡ã—ã€from_encodingã§Shift-JISã‚’æŒ‡å®š
            soup = BeautifulSoup(response.content, 'html.parser', from_encoding='shift_jis')

            # ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™ï¼ˆãƒ‡ãƒ¼ã‚¿ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’ç‰¹å®šï¼‰
            # WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã®ãƒãƒŠãƒ¼ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’é¿ã‘ã‚‹ãŸã‚ã€border="1"ã‚’æŒã¤ãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            table = soup.find('table', border="1")

            # borderå±æ€§ãŒãªã„å ´åˆã€ä¸­å¤®ã«é…ç½®ã•ã‚ŒãŸãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æ¢ã™
            if not table:
                center = soup.find('center')
                if center:
                    table = center.find('table')

            if not table:
                logger.warning(f"âš ï¸  ãƒ†ãƒ¼ãƒ–ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {url}")
                return []

            bonds = []
            rows = table.find_all('tr')[1:]  # ãƒ˜ãƒƒãƒ€ãƒ¼è¡Œã‚’ã‚¹ã‚­ãƒƒãƒ—

            for row in rows:
                cells = row.find_all('td')
                if len(cells) >= 3:
                    bond_type = cells[0].get_text(strip=True)
                    issue_number_str = cells[1].get_text(strip=True)
                    amount_str = cells[2].get_text(strip=True)

                    # å›å·ã‚’æŠ½å‡º
                    issue_number = int(issue_number_str) if issue_number_str.isdigit() else None

                    # ç™ºè¡Œé¡ã‚’æŠ½å‡ºï¼ˆã‚«ãƒ³ãƒé™¤å»ï¼‰
                    amount = float(amount_str.replace(',', '')) if amount_str.replace(',', '').replace('.', '').isdigit() else None

                    if issue_number and amount:
                        bonds.append({
                            'bond_type': bond_type,
                            'issue_number': issue_number,
                            'amount': amount
                        })

            if bonds:
                logger.info(f"âœ… {len(bonds)} éŠ˜æŸ„ã‚’æŠ½å‡º")
            return bonds

        except Exception as e:
            self._last_error = str(e)
            logger.error(f"âŒ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _scrape_from_warp_archive(self, original_url: str) -> List[Dict[str, Any]]:
        """
        å›½ç«‹å›½ä¼šå›³æ›¸é¤¨WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰éŠ˜æŸ„ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—

        Args:
            original_url: å…ƒã®è²¡å‹™çœURL

        Returns:
            ç™ºè¡ŒéŠ˜æŸ„ã®ãƒªã‚¹ãƒˆ
        """
        # å…ƒã®URLã‹ã‚‰ãƒ‘ã‚¹éƒ¨åˆ†ã‚’æŠ½å‡º
        # ä¾‹: https://www.mof.go.jp/jgbs/auction/calendar/nyusatsu/resul20190822a.htm
        # â†’ www.mof.go.jp/jgbs/auction/calendar/nyusatsu/resul20190822a.htm
        if original_url.startswith('https://'):
            url_path = original_url.replace('https://', '')
        elif original_url.startswith('http://'):
            url_path = original_url.replace('http://', '')
        else:
            url_path = original_url

        # è¤‡æ•°ã®pidã‚’è©¦ã™
        for pid in self.WARP_PIDS:
            warp_url = f"https://warp.da.ndl.go.jp/info:ndljp/pid/{pid}/{url_path}"
            logger.info(f"  ğŸ” WARP (pid={pid}): {warp_url}")

            bonds = self._scrape_from_url(warp_url)
            if bonds:
                logger.info(f"  âœ… WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã‹ã‚‰å–å¾—æˆåŠŸ (pid={pid})")
                return bonds

        logger.warning(f"  âš ï¸  å…¨ã¦ã®WARPã‚¢ãƒ¼ã‚«ã‚¤ãƒ–ã§å–å¾—å¤±æ•—")
        return []

    def generate_bond_code(self, issue_number: int, bond_type: str) -> Optional[str]:
        """
        éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ã‚’ç”Ÿæˆ

        Args:
            issue_number: å›å·
            bond_type: å‚µåˆ¸ç¨®é¡ï¼ˆä¾‹: "2å¹´å‚µ", "5å¹´å‚µ"ï¼‰

        Returns:
            éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ï¼ˆ9æ¡ï¼‰
        """
        # å‚µåˆ¸ç¨®é¡ã‹ã‚‰ç¨®é¡ã‚³ãƒ¼ãƒ‰ã‚’å–å¾—
        type_code_map = {
            '2å¹´å‚µ': '0042',
            '4å¹´å‚µ': '0061',
            '5å¹´å‚µ': '0045',
            '6å¹´å‚µ': '0061',
            '10å¹´å‚µ': '0067',
            '20å¹´å‚µ': '0069',
            '30å¹´å‚µ': '0068',
            '40å¹´å‚µ': '0054',
        }

        type_code = type_code_map.get(bond_type)
        if not type_code:
            logger.warning(f"âš ï¸  æœªå¯¾å¿œã®å‚µåˆ¸ç¨®é¡: {bond_type}")
            return None

        # éŠ˜æŸ„ã‚³ãƒ¼ãƒ‰ = å›å·ï¼ˆ5æ¡0ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼‰ + ç¨®é¡ã‚³ãƒ¼ãƒ‰ï¼ˆ4æ¡ï¼‰
        bond_code = f"{issue_number:05d}{type_code}"
        return bond_code

    def collect_all_data(self) -> List[Dict[str, Any]]:
        """
        å…¨ãƒ‡ãƒ¼ã‚¿ã‚’åé›†ï¼ˆExcel + Webã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ï¼‰

        Returns:
            å„éŠ˜æŸ„ã®è©³ç´°ãƒ‡ãƒ¼ã‚¿ãƒªã‚¹ãƒˆ
        """
        logger.info("=" * 70)
        logger.info("æµå‹•æ€§ä¾›çµ¦å…¥æœ­ãƒ‡ãƒ¼ã‚¿åé›†é–‹å§‹")
        logger.info("=" * 70)

        # Step 1: Excelãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰
        excel_path = self.download_excel()

        # Step 2: ã‚µãƒãƒªãƒ¼æŠ½å‡º
        summaries = self.parse_excel_summary(excel_path)

        # Step 3: å„ã‚µãƒãƒªãƒ¼ã«ã¤ã„ã¦è¿½åŠ ç™ºè¡ŒéŠ˜æŸ„ã‚’å–å¾—
        all_bonds = []

        for summary in summaries:
            logger.info(f"\nğŸ“‹ ç¬¬{summary['round_number']}å› ({summary['auction_date']})")

            # URLã‚’ç”Ÿæˆ
            url = self.extract_hyperlink_from_excel(
                excel_path,
                summary['round_number'],
                summary['auction_date']
            )

            if not url:
                logger.warning(f"âš ï¸  URLç”Ÿæˆå¤±æ•—ï¼ˆç¬¬{summary['round_number']}å›ï¼‰")
                continue

            # è¿½åŠ ç™ºè¡ŒéŠ˜æŸ„ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°
            issued_bonds = self.scrape_issued_bonds(url)

            # å„éŠ˜æŸ„ã«ã‚µãƒãƒªãƒ¼æƒ…å ±ã‚’ä»˜åŠ 
            for bond in issued_bonds:
                bond_code = self.generate_bond_code(bond['issue_number'], bond['bond_type'])

                if not bond_code:
                    continue

                bond_data = {
                    # ä¸»ã‚­ãƒ¼
                    'bond_code': bond_code,
                    'auction_date': summary['auction_date'],

                    # åŸºæœ¬æƒ…å ±
                    'issue_number': bond['issue_number'],
                    'bond_type_label': bond['bond_type'],

                    # ç™ºè¡Œé¡ï¼ˆå€‹åˆ¥éŠ˜æŸ„ï¼‰
                    'allocated_amount': bond['amount'],

                    # åˆ©å›ã‚Šï¼ˆåˆ©å›æ ¼å·®ï¼‰
                    'highest_yield': summary['highest_yield'],  # æœ€å¤§åˆ©å›æ ¼å·®
                    'average_yield': summary['average_yield'],  # å¹³å‡åˆ©å›æ ¼å·®

                    # ã‚µãƒãƒªãƒ¼æƒ…å ±
                    'liquidity_round': summary['round_number'],
                    'issue_date': summary['issue_date'],
                    'total_planned_amount': summary['planned_amount'],
                    'total_offered_amount': summary['offered_amount'],
                    'total_allocated_amount': summary['allocated_amount'],
                    'prorate_ratio': summary['prorate_ratio'],

                    # auction_type
                    'auction_type': 'tap'
                }

                all_bonds.append(bond_data)

        logger.info("\n" + "=" * 70)
        logger.info(f"âœ… åé›†å®Œäº†: åˆè¨ˆ {len(all_bonds)} éŠ˜æŸ„")
        logger.info("=" * 70)

        return all_bonds

    def _parse_amount(self, value: Any) -> Optional[float]:
        """é‡‘é¡ãƒ‘ãƒ¼ã‚¹ï¼ˆå„„å††å˜ä½ï¼‰"""
        if pd.isna(value):
            return None

        try:
            # "6,000å„„å††" â†’ 6000.0
            amount_str = str(value).replace(',', '').replace('å„„å††', '').replace('å…†å††', '0000').strip()
            return float(amount_str)
        except:
            return None

    def _parse_percentage(self, value: Any) -> Optional[float]:
        """ãƒ‘ãƒ¼ã‚»ãƒ³ãƒ†ãƒ¼ã‚¸ãƒ‘ãƒ¼ã‚¹"""
        if pd.isna(value):
            return None

        try:
            # "-0.015%" â†’ -0.015
            pct_str = str(value).replace('%', '').strip()
            return float(pct_str)
        except:
            return None


if __name__ == "__main__":
    # ãƒ†ã‚¹ãƒˆå®Ÿè¡Œ
    collector = LiquiditySupplyCollector()

    # æœ€æ–°ãƒ‡ãƒ¼ã‚¿ã®ã¿ãƒ†ã‚¹ãƒˆ
    print("\nğŸ§ª ãƒ†ã‚¹ãƒˆ: æœ€æ–°3å›åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’åé›†\n")

    excel_path = collector.download_excel()
    summaries = collector.parse_excel_summary(excel_path)

    # æœ€æ–°1ä»¶ã®ã¿ãƒ†ã‚¹ãƒˆ
    for summary in summaries[-1:]:
        print(f"\nç¬¬{summary['round_number']}å›:")
        print(f"  å…¥æœ­æ—¥: {summary['auction_date']}")
        print(f"  ç™ºè¡Œé¡: {summary['allocated_amount']}å„„å††")

        url = collector.extract_hyperlink_from_excel(
            excel_path,
            summary['round_number'],
            summary['auction_date']
        )
        if url:
            print(f"  URL: {url}")
            bonds = collector.scrape_issued_bonds(url)
            print(f"  ç™ºè¡ŒéŠ˜æŸ„: {len(bonds)}ä»¶")

            # 20å¹´å‚µç¬¬140å›ã‚’æ¢ã™
            bond_140 = None
            for bond in bonds:
                if '20å¹´å‚µ' in bond['bond_type'] and bond['issue_number'] == 140:
                    bond_140 = bond
                    break

            if bond_140:
                bond_code = collector.generate_bond_code(bond_140['issue_number'], bond_140['bond_type'])
                print(f"\n  âœ… 20å¹´å‚µç¬¬140å›ã‚’ç™ºè¦‹:")
                print(f"     ç™ºè¡Œé¡: {bond_140['amount']}å„„å††")
                print(f"     bond_code: {bond_code}")
            else:
                print(f"\n  âš ï¸ 20å¹´å‚µç¬¬140å›ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")

            # å…¨éŠ˜æŸ„ã‚’è¡¨ç¤º
            print("\n  å…¨éŠ˜æŸ„:")
            for i, bond in enumerate(bonds, 1):
                bond_code = collector.generate_bond_code(bond['issue_number'], bond['bond_type'])
                print(f"    {i:2d}. {bond['bond_type']:8s} ç¬¬{bond['issue_number']:3d}å› "
                      f"{bond['amount']:6.0f}å„„å†† â†’ {bond_code}")
