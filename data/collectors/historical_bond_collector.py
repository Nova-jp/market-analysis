#!/usr/bin/env python3
"""
Historical Bond Data Collector
JSDAã‹ã‚‰ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—ã™ã‚‹ã‚¯ãƒ©ã‚¹
"""

import pandas as pd
import requests
import io
import time
import logging
from datetime import datetime, timedelta, date
from typing import Optional, List, Dict, Tuple
import os
from dotenv import load_dotenv
import calendar

load_dotenv()

class HistoricalBondCollector:
    """JSDAãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, delay_seconds: float = 1.0):
        self.base_url = "https://market.jsda.or.jp/shijyo/saiken/baibai/baisanchi/files"
        self.delay_seconds = delay_seconds  # ãƒªã‚¯ã‚¨ã‚¹ãƒˆé–“éš”
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_key = os.getenv('SUPABASE_KEY')
        
        # ãƒ­ã‚°è¨­å®š
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s'
        )
        self.logger = logging.getLogger(__name__)
        
        # æ—¥æœ¬ã®ç¥æ—¥ï¼ˆç°¡æ˜“ç‰ˆï¼‰
        self.holidays = self._get_japanese_holidays()
        
    def _get_japanese_holidays(self) -> List[date]:
        """æ—¥æœ¬ã®ç¥æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—ï¼ˆç°¡æ˜“ç‰ˆï¼‰"""
        holidays = []
        
        # ä¸»è¦ãªç¥æ—¥ã®ã¿ï¼ˆå…ƒæ—¥ã€GWã€ãŠç›†ã€å¹´æœ«å¹´å§‹ãªã©ï¼‰
        for year in range(2002, 2026):
            holidays.extend([
                date(year, 1, 1),   # å…ƒæ—¥
                date(year, 1, 2),   # æ­£æœˆä¼‘ã¿
                date(year, 1, 3),   # æ­£æœˆä¼‘ã¿
                date(year, 5, 3),   # æ†²æ³•è¨˜å¿µæ—¥
                date(year, 5, 4),   # ã¿ã©ã‚Šã®æ—¥
                date(year, 5, 5),   # ã“ã©ã‚‚ã®æ—¥
                date(year, 12, 29), # å¹´æœ«ä¼‘ã¿
                date(year, 12, 30), # å¹´æœ«ä¼‘ã¿
                date(year, 12, 31), # å¤§æ™¦æ—¥
            ])
        
        return holidays
    
    def is_business_day(self, check_date: date) -> bool:
        """å–¶æ¥­æ—¥ã‹ã©ã†ã‹ã‚’åˆ¤å®š"""
        # åœŸæ—¥ã‚’ãƒã‚§ãƒƒã‚¯
        if check_date.weekday() >= 5:  # åœŸæ›œæ—¥(5)ã€æ—¥æ›œæ—¥(6)
            return False
        
        # ç¥æ—¥ã‚’ãƒã‚§ãƒƒã‚¯
        if check_date in self.holidays:
            return False
            
        return True
    
    def get_business_days_in_range(self, start_date: date, end_date: date) -> List[date]:
        """æŒ‡å®šæœŸé–“ã®å–¶æ¥­æ—¥ãƒªã‚¹ãƒˆã‚’å–å¾—"""
        business_days = []
        current_date = start_date
        
        while current_date <= end_date:
            if self.is_business_day(current_date):
                business_days.append(current_date)
            current_date += timedelta(days=1)
        
        return business_days
    
    def build_csv_url(self, target_date: date) -> Tuple[str, str]:
        """æŒ‡å®šæ—¥ã®CSVãƒ•ã‚¡ã‚¤ãƒ«URLã‚’æ§‹ç¯‰"""
        year = target_date.year
        date_str = target_date.strftime("%y%m%d")
        filename = f"S{date_str}.csv"
        url = f"{self.base_url}/{year}/{filename}"
        return url, filename
    
    def download_csv_data(self, url: str, target_date: date) -> Optional[pd.DataFrame]:
        """CSVãƒ‡ãƒ¼ã‚¿ã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰"""
        try:
            self.logger.info(f"ãƒ‡ãƒ¼ã‚¿å–å¾—ä¸­: {target_date.strftime('%Y-%m-%d')} - {url}")
            
            response = requests.get(url, timeout=30)
            response.raise_for_status()
            
            # æ–‡å­—ã‚³ãƒ¼ãƒ‰åˆ¤å®šãƒ»å¤‰æ›
            try:
                content = response.content.decode('shift_jis')
            except UnicodeDecodeError:
                try:
                    content = response.content.decode('utf-8')
                except UnicodeDecodeError:
                    content = response.content.decode('cp932')
            
            # CSVã¨ã—ã¦ãƒ‘ãƒ¼ã‚¹
            df = pd.read_csv(io.StringIO(content), header=None)
            
            if len(df) > 0:
                self.logger.info(f"å–å¾—æˆåŠŸ: {len(df)}è¡Œ")
                return df
            else:
                self.logger.warning(f"ç©ºã®ãƒ•ã‚¡ã‚¤ãƒ«: {target_date}")
                return None
                
        except requests.exceptions.HTTPError as e:
            if e.response.status_code == 404:
                self.logger.info(f"ãƒ‡ãƒ¼ã‚¿ãªã—ï¼ˆ404ï¼‰: {target_date}")
            else:
                self.logger.error(f"HTTPã‚¨ãƒ©ãƒ¼ {e.response.status_code}: {target_date}")
        except requests.exceptions.RequestException as e:
            self.logger.error(f"æ¥ç¶šã‚¨ãƒ©ãƒ¼: {target_date} - {e}")
        except Exception as e:
            self.logger.error(f"äºˆæœŸã—ãªã„ã‚¨ãƒ©ãƒ¼: {target_date} - {e}")
        
        return None
    
    def get_existing_dates(self) -> List[date]:
        """ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†…ã®æ—¢å­˜æ—¥ä»˜ã‚’å–å¾—"""
        if not self.supabase_url or not self.supabase_key:
            self.logger.warning("Supabaseè¨­å®šãŒã‚ã‚Šã¾ã›ã‚“")
            return []
        
        headers = {
            'apikey': self.supabase_key,
            'Authorization': f'Bearer {self.supabase_key}'
        }
        
        try:
            response = requests.get(
                f"{self.supabase_url}/rest/v1/clean_bond_data?select=trade_date&order=trade_date",
                headers=headers
            )
            
            if response.status_code == 200:
                data = response.json()
                existing_dates = [datetime.strptime(item['trade_date'], '%Y-%m-%d').date() 
                                for item in data]
                unique_dates = list(set(existing_dates))
                self.logger.info(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿: {len(unique_dates)}æ—¥åˆ†")
                return unique_dates
            else:
                self.logger.error(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèªã‚¨ãƒ©ãƒ¼: {response.status_code}")
                
        except Exception as e:
            self.logger.error(f"æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèªå¤±æ•—: {e}")
        
        return []
    
    def collect_historical_data(self, start_date: date, end_date: date, 
                              skip_existing: bool = True) -> Dict[str, int]:
        """ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿ã®ä¸€æ‹¬å–å¾—"""
        
        self.logger.info(f"ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—é–‹å§‹: {start_date} to {end_date}")
        
        # å–¶æ¥­æ—¥ãƒªã‚¹ãƒˆä½œæˆ
        business_days = self.get_business_days_in_range(start_date, end_date)
        total_days = len(business_days)
        
        self.logger.info(f"å¯¾è±¡å–¶æ¥­æ—¥æ•°: {total_days}æ—¥")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        existing_dates = set()
        if skip_existing:
            existing_dates = set(self.get_existing_dates())
            if existing_dates:
                business_days = [d for d in business_days if d not in existing_dates]
                self.logger.info(f"æœªå–å¾—å–¶æ¥­æ—¥æ•°: {len(business_days)}æ—¥")
        
        # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿
        stats = {
            'attempted': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_records': 0
        }
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨ï¼ˆæ—¢å­˜ã®processorã‚’ä½¿ç”¨ï¼‰
        from ..processors.bond_data_processor import BondDataProcessor
        processor = BondDataProcessor()
        
        # ä¸€æ—¥ãšã¤å‡¦ç†
        for i, target_date in enumerate(business_days):
            stats['attempted'] += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            progress = (i + 1) / len(business_days) * 100
            self.logger.info(f"é€²è¡ŒçŠ¶æ³: {progress:.1f}% ({i+1}/{len(business_days)})")
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—
            url, filename = self.build_csv_url(target_date)
            raw_df = self.download_csv_data(url, target_date)
            
            if raw_df is not None:
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                processed_df = processor.process_raw_data(raw_df)
                
                if not processed_df.empty:
                    # æ—¥ä»˜ã‚’æ­£ã—ãè¨­å®š
                    processed_df['trade_date'] = target_date
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                    if processor.save_to_supabase(processed_df):
                        stats['successful'] += 1
                        stats['total_records'] += len(processed_df)
                        self.logger.info(f"ä¿å­˜æˆåŠŸ: {target_date} ({len(processed_df)}ä»¶)")
                    else:
                        stats['failed'] += 1
                        self.logger.error(f"ä¿å­˜å¤±æ•—: {target_date}")
                else:
                    stats['skipped'] += 1
                    self.logger.warning(f"å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãªã—: {target_date}")
            else:
                stats['failed'] += 1
            
            # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆã‚µãƒ¼ãƒãƒ¼è² è·è»½æ¸›ï¼‰
            if i < len(business_days) - 1:  # æœ€å¾Œä»¥å¤–
                time.sleep(self.delay_seconds)
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆ
        self.logger.info("=" * 50)
        self.logger.info("ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—å®Œäº†")
        self.logger.info(f"è©¦è¡Œ: {stats['attempted']}æ—¥")
        self.logger.info(f"æˆåŠŸ: {stats['successful']}æ—¥")
        self.logger.info(f"å¤±æ•—: {stats['failed']}æ—¥")
        self.logger.info(f"ã‚¹ã‚­ãƒƒãƒ—: {stats['skipped']}æ—¥")
        self.logger.info(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['total_records']}ä»¶")
        self.logger.info("=" * 50)
        
        return stats
    
    def collect_historical_data_with_adaptive_delay(self, start_date: date, end_date: date, 
                                                  batch_size: int = 10, initial_delay: float = 30,
                                                  skip_existing: bool = True) -> Dict[str, int]:
        """é©å¿œçš„é…å»¶æ©Ÿèƒ½ä»˜ããƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†"""
        
        self.logger.info(f"é©å¿œçš„é…å»¶åé›†é–‹å§‹: {start_date} to {end_date}, åˆæœŸé–“éš”: {initial_delay}ç§’")
        
        # å–¶æ¥­æ—¥ãƒªã‚¹ãƒˆä½œæˆ
        business_days = self.get_business_days_in_range(start_date, end_date)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        existing_dates = set()
        if skip_existing:
            existing_dates = set(self.get_existing_dates())
            if existing_dates:
                business_days = [d for d in business_days if d not in existing_dates]
                self.logger.info(f"æœªå–å¾—å–¶æ¥­æ—¥æ•°: {len(business_days)}æ—¥")
        
        # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿
        stats = {
            'attempted': 0,
            'successful': 0,
            'failed': 0,
            'timeout_errors': 0,
            'connection_errors': 0,
            'total_records': 0,
            'delay_adjustments': 0
        }
        
        # é©å¿œçš„é…å»¶ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        current_delay = initial_delay
        consecutive_failures = 0
        consecutive_successes = 0
        
        # é…å»¶èª¿æ•´ãƒ«ãƒ¼ãƒ«
        delay_levels = [15, 20, 30, 45, 60, 90, 120, 180, 300]  # 15ç§’ã‹ã‚‰5åˆ†ã¾ã§
        success_threshold = 3  # é€£ç¶šæˆåŠŸã§ãƒ¬ãƒ™ãƒ«ä¸‹ã’
        failure_threshold = 2  # é€£ç¶šå¤±æ•—ã§ãƒ¬ãƒ™ãƒ«ä¸Šã’
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨
        from ..processors.bond_data_processor import BondDataProcessor
        processor = BondDataProcessor()
        
        self.logger.info(f"ğŸ“Š å¯¾è±¡æ—¥æ•°: {len(business_days)}æ—¥")
        self.logger.info(f"âš¡ é©å¿œçš„é…å»¶: {delay_levels}ç§’ã®ãƒ¬ãƒ™ãƒ«èª¿æ•´")
        
        # æ—¥ä»˜ã‚’é †ç•ªã«å‡¦ç†
        for i, target_date in enumerate(business_days):
            stats['attempted'] += 1
            
            # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
            progress = (i + 1) / len(business_days) * 100
            self.logger.info(f"ğŸ“Š é€²è¡ŒçŠ¶æ³: {progress:.1f}% ({i+1}/{len(business_days)}) | ç¾åœ¨é–“éš”: {current_delay}ç§’")
            
            # ãƒ‡ãƒ¼ã‚¿å–å¾—è©¦è¡Œ
            url, filename = self.build_csv_url(target_date)
            raw_df = self.download_csv_data(url, target_date)
            
            success = False
            
            if raw_df is not None:
                # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                processed_df = processor.process_raw_data(raw_df)
                
                if not processed_df.empty:
                    # æ—¥ä»˜ã‚’æ­£ã—ãè¨­å®š
                    processed_df['trade_date'] = target_date
                    
                    # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                    if processor.save_to_supabase(processed_df):
                        stats['successful'] += 1
                        stats['total_records'] += len(processed_df)
                        success = True
                        self.logger.info(f"âœ… ä¿å­˜æˆåŠŸ: {target_date} ({len(processed_df)}ä»¶)")
                    else:
                        stats['failed'] += 1
                        self.logger.error(f"âŒ ä¿å­˜å¤±æ•—: {target_date}")
                else:
                    success = True  # ãƒ‡ãƒ¼ã‚¿ãªã—ã‚‚æˆåŠŸæ‰±ã„
                    self.logger.warning(f"â­ï¸  å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãªã—: {target_date}")
            else:
                stats['failed'] += 1
                self.logger.error(f"âŒ ãƒ‡ãƒ¼ã‚¿å–å¾—å¤±æ•—: {target_date}")
            
            # é©å¿œçš„é…å»¶èª¿æ•´
            if success:
                consecutive_successes += 1
                consecutive_failures = 0
                
                # é€£ç¶šæˆåŠŸã§é…å»¶çŸ­ç¸®
                if consecutive_successes >= success_threshold:
                    current_level = delay_levels.index(current_delay) if current_delay in delay_levels else len(delay_levels) - 1
                    if current_level > 0:
                        current_delay = delay_levels[current_level - 1]
                        stats['delay_adjustments'] += 1
                        self.logger.info(f"ğŸš€ é…å»¶çŸ­ç¸®: {current_delay}ç§’ (é€£ç¶šæˆåŠŸ: {consecutive_successes})")
                        consecutive_successes = 0
            else:
                consecutive_failures += 1
                consecutive_successes = 0
                
                # é€£ç¶šå¤±æ•—ã«ã‚ˆã‚‹é…å»¶èª¿æ•´
                if consecutive_failures >= failure_threshold:
                    # ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆãŒç™ºç”Ÿã—ãŸå ´åˆã¯å³åº§ã«2åˆ†ä¼‘æ†©
                    if consecutive_failures >= 3:  # 3å›é€£ç¶šå¤±æ•—ã§2åˆ†ä¼‘æ†©
                        self.logger.warning("ğŸ’¤ ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆå¤šç™ºæ¤œçŸ¥: 2åˆ†é–“ä¼‘æ†©ã—ã¦ãƒªã‚»ãƒƒãƒˆã—ã¾ã™...")
                        time.sleep(120)  # 2åˆ†ä¼‘æ†©
                        current_delay = 15  # 15ç§’ã§ãƒªã‚»ãƒƒãƒˆ
                        consecutive_failures = 0
                        consecutive_successes = 0
                        stats['delay_adjustments'] += 1
                        self.logger.info("ğŸ”„ ä¼‘æ†©å®Œäº†: 15ç§’é–“éš”ã§ãƒªã‚»ãƒƒãƒˆå†é–‹")
                    else:
                        # é€šå¸¸ã®é€£ç¶šå¤±æ•—å‡¦ç†
                        current_level = delay_levels.index(current_delay) if current_delay in delay_levels else 0
                        if current_level < len(delay_levels) - 1:
                            current_delay = delay_levels[current_level + 1]
                            stats['delay_adjustments'] += 1
                            self.logger.warning(f"â° é…å»¶å»¶é•·: {current_delay}ç§’ (é€£ç¶šå¤±æ•—: {consecutive_failures})")
                            consecutive_failures = 0
            
            # æ¬¡ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆã¾ã§å¾…æ©Ÿ
            if i < len(business_days) - 1:
                self.logger.info(f"â±ï¸  {current_delay}ç§’å¾…æ©Ÿä¸­...")
                time.sleep(current_delay)
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆ
        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ é©å¿œçš„é…å»¶åé›†å®Œäº†")
        self.logger.info(f"ğŸ¯ è©¦è¡Œ: {stats['attempted']}æ—¥")
        self.logger.info(f"âœ… æˆåŠŸ: {stats['successful']}æ—¥")
        self.logger.info(f"âŒ å¤±æ•—: {stats['failed']}æ—¥")
        self.logger.info(f"â° ã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ: {stats['timeout_errors']}ä»¶")
        self.logger.info(f"ğŸ”Œ æ¥ç¶šã‚¨ãƒ©ãƒ¼: {stats['connection_errors']}ä»¶") 
        self.logger.info(f"ğŸ’¾ ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['total_records']}ä»¶")
        self.logger.info(f"âš¡ é…å»¶èª¿æ•´å›æ•°: {stats['delay_adjustments']}å›")
        self.logger.info(f"ğŸ¯ æœ€çµ‚é–“éš”: {current_delay}ç§’")
        self.logger.info("=" * 60)
        
        return stats

    def collect_historical_data_in_batches(self, start_date: date, end_date: date, 
                                         batch_size: int = 10, delay_seconds: float = 300,
                                         skip_existing: bool = True) -> Dict[str, int]:
        """ãƒãƒƒãƒå‡¦ç†ã§ã®ãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿åé›†ï¼ˆä¸­æ–­ãƒ»å†é–‹å¯¾å¿œï¼‰"""
        
        self.logger.info(f"ãƒãƒƒãƒå‡¦ç†é–‹å§‹: {start_date} to {end_date}, ãƒãƒƒãƒã‚µã‚¤ã‚º: {batch_size}")
        
        # å–¶æ¥­æ—¥ãƒªã‚¹ãƒˆä½œæˆ
        business_days = self.get_business_days_in_range(start_date, end_date)
        total_days = len(business_days)
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ç¢ºèª
        existing_dates = set()
        if skip_existing:
            existing_dates = set(self.get_existing_dates())
            if existing_dates:
                business_days = [d for d in business_days if d not in existing_dates]
                self.logger.info(f"æœªå–å¾—å–¶æ¥­æ—¥æ•°: {len(business_days)}æ—¥")
        
        # çµ±è¨ˆã‚«ã‚¦ãƒ³ã‚¿
        stats = {
            'attempted': 0,
            'successful': 0,
            'failed': 0,
            'skipped': 0,
            'total_records': 0,
            'batches_completed': 0
        }
        
        # ãƒ‡ãƒ¼ã‚¿å‡¦ç†ç”¨
        from ..processors.bond_data_processor import BondDataProcessor
        processor = BondDataProcessor()
        
        # ãƒãƒƒãƒå˜ä½ã§å‡¦ç†
        for batch_start in range(0, len(business_days), batch_size):
            batch_end = min(batch_start + batch_size, len(business_days))
            batch_dates = business_days[batch_start:batch_end]
            current_batch = (batch_start // batch_size) + 1
            total_batches = (len(business_days) + batch_size - 1) // batch_size
            
            self.logger.info(f"=" * 60)
            self.logger.info(f"ğŸ“¦ ãƒãƒƒãƒ {current_batch}/{total_batches} é–‹å§‹")
            self.logger.info(f"ğŸ“… æœŸé–“: {batch_dates[0]} ï½ {batch_dates[-1]} ({len(batch_dates)}æ—¥)")
            self.logger.info(f"=" * 60)
            
            batch_success = 0
            
            # ãƒãƒƒãƒå†…ã®å„æ—¥ä»˜ã‚’å‡¦ç†
            for i, target_date in enumerate(batch_dates):
                stats['attempted'] += 1
                
                # ãƒ—ãƒ­ã‚°ãƒ¬ã‚¹è¡¨ç¤º
                batch_progress = (i + 1) / len(batch_dates) * 100
                overall_progress = (batch_start + i + 1) / len(business_days) * 100
                
                self.logger.info(f"ğŸ“Š ãƒãƒƒãƒé€²è¡Œ: {batch_progress:.1f}% | å…¨ä½“: {overall_progress:.1f}% ({batch_start + i + 1}/{len(business_days)})")
                
                # ãƒ‡ãƒ¼ã‚¿å–å¾—
                url, filename = self.build_csv_url(target_date)
                raw_df = self.download_csv_data(url, target_date)
                
                if raw_df is not None:
                    # ãƒ‡ãƒ¼ã‚¿å‡¦ç†
                    processed_df = processor.process_raw_data(raw_df)
                    
                    if not processed_df.empty:
                        # æ—¥ä»˜ã‚’æ­£ã—ãè¨­å®š
                        processed_df['trade_date'] = target_date
                        
                        # ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹ä¿å­˜
                        if processor.save_to_supabase(processed_df):
                            stats['successful'] += 1
                            stats['total_records'] += len(processed_df)
                            batch_success += 1
                            self.logger.info(f"âœ… ä¿å­˜æˆåŠŸ: {target_date} ({len(processed_df)}ä»¶)")
                        else:
                            stats['failed'] += 1
                            self.logger.error(f"âŒ ä¿å­˜å¤±æ•—: {target_date}")
                    else:
                        stats['skipped'] += 1
                        self.logger.warning(f"â­ï¸  å‡¦ç†ãƒ‡ãƒ¼ã‚¿ãªã—: {target_date}")
                else:
                    stats['failed'] += 1
                
                # ãƒ¬ãƒ¼ãƒˆåˆ¶é™ï¼ˆæœ€å¾Œã®æ—¥ä»˜ä»¥å¤–ï¼‰
                if i < len(batch_dates) - 1:
                    self.logger.info(f"â±ï¸  {delay_seconds}ç§’å¾…æ©Ÿä¸­...")
                    time.sleep(delay_seconds)
            
            # ãƒãƒƒãƒå®Œäº†
            stats['batches_completed'] += 1
            self.logger.info(f"ğŸ¯ ãƒãƒƒãƒ {current_batch} å®Œäº†: {batch_success}/{len(batch_dates)}æ—¥æˆåŠŸ")
            
            # æœ€å¾Œã®ãƒãƒƒãƒã§ãªã„å ´åˆã€æ¬¡ã®ãƒãƒƒãƒã¾ã§å°‘ã—å¾…æ©Ÿ
            if batch_end < len(business_days):
                extra_wait = 60  # 1åˆ†è¿½åŠ å¾…æ©Ÿ
                self.logger.info(f"ğŸ’¤ æ¬¡ã®ãƒãƒƒãƒã¾ã§{extra_wait}ç§’å¾…æ©Ÿ...")
                time.sleep(extra_wait)
        
        # çµæœãƒ¬ãƒãƒ¼ãƒˆ
        self.logger.info("=" * 60)
        self.logger.info("ğŸ‰ ãƒãƒƒãƒå‡¦ç†å®Œäº†")
        self.logger.info(f"ğŸ“¦ å®Œäº†ãƒãƒƒãƒ: {stats['batches_completed']}")
        self.logger.info(f"ğŸ¯ è©¦è¡Œ: {stats['attempted']}æ—¥")
        self.logger.info(f"âœ… æˆåŠŸ: {stats['successful']}æ—¥")
        self.logger.info(f"âŒ å¤±æ•—: {stats['failed']}æ—¥")
        self.logger.info(f"â­ï¸  ã‚¹ã‚­ãƒƒãƒ—: {stats['skipped']}æ—¥")
        self.logger.info(f"ğŸ’¾ ç·ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°: {stats['total_records']}ä»¶")
        self.logger.info("=" * 60)
        
        return stats

def main():
    """ãƒ¡ã‚¤ãƒ³å®Ÿè¡Œ"""
    collector = HistoricalBondCollector(delay_seconds=1.5)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆ: 2020å¹´ã‹ã‚‰ã®5å¹´åˆ†ã®ãƒ‡ãƒ¼ã‚¿
    start_date = date(2020, 1, 1)
    end_date = date.today()
    
    print(f"JSDAãƒ’ã‚¹ãƒˆãƒªã‚«ãƒ«ãƒ‡ãƒ¼ã‚¿å–å¾—")
    print(f"æœŸé–“: {start_date} ï½ {end_date}")
    
    # ãƒ¦ãƒ¼ã‚¶ãƒ¼ç¢ºèª
    response = input("ã“ã®æœŸé–“ã§ãƒ‡ãƒ¼ã‚¿å–å¾—ã‚’é–‹å§‹ã—ã¾ã™ã‹ï¼Ÿ (y/n): ")
    if response.lower() != 'y':
        print("å–å¾—ã‚’ã‚­ãƒ£ãƒ³ã‚»ãƒ«ã—ã¾ã—ãŸ")
        return
    
    # ãƒ‡ãƒ¼ã‚¿å–å¾—å®Ÿè¡Œ
    stats = collector.collect_historical_data(start_date, end_date)
    
    print("\nğŸ‰ å–å¾—å®Œäº†ï¼")
    print(f"æˆåŠŸ: {stats['successful']}æ—¥")
    print(f"ç·ãƒ¬ã‚³ãƒ¼ãƒ‰: {stats['total_records']}ä»¶")

if __name__ == "__main__":
    main()